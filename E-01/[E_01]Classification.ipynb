{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[E-01]Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP7xAq6aWQU01tiK2z8w5e2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lemonbuilder/EXPLORATION/blob/main/E-01/%5BE_01%5DClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [E-01] Classification\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "< 목 차 >\n",
        "\n",
        "####I.   서 론\n",
        "\n",
        "####II.  본 론\n",
        "\n",
        "1.   load_digits: 손글씨 이미지 판별\n",
        "2.   load_wine: 와인 종류 판별\n",
        "3.   load_breast_cancer: 유방암 여부 진단\n",
        "\n",
        "####III. 결 론\n",
        "\n",
        "<br><br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "leVYjy_xVvHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **I.  서 론**\n",
        "\n",
        "<br>\n",
        "\n",
        "###**[배경 및 의의]**\n",
        "\n",
        "머신러닝의 알고리즘 종류는 크게 3가지로 나눌 수 있다. 지도학습(Supervised Learning), 비지도학습(Unsupervised Learning), 강화학습(Reinforcement Learning)으로 구분되는 알고리즘을 이해하는 것은 이를 통해 문제를 해결하기 위한 출발점이라 할 수 있다. 또한 향후에 다른 알고리즘을 이해하는 데에도 기반이 될 것이다. \n",
        "\n",
        "첫 EXPLORATION에서는 머신러닝의 알고리즘 체계를 이해하고, 그 중 지도학습에 속하는 분류(Classification) 알고리즘을 활용해봄으로써 범주형 변수에 대한 예측을 실시하고자 한다. \n",
        "\n",
        "첫 단계인 만큼 완벽한 문제해결을 목표로 하기보다는 결과를 합리적으로 해석하고 더 나은 대안을 찾는 방식을 고민하는 것에 중점을 두었다. 그 과정에서 머신러닝 라이브러리를 이해하고 적합하게 활용할 수 있는 기본소양이 쌓이기를 기대한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "###**[목표]**\n",
        "*   머신러닝 라이브러리를 활용한 데이터 분석방법 숙지\n",
        "*   분류에 대한 이해와 이를 통한 문제해결력 함양\n",
        "*   학습모델의 활용\n",
        "*   결과평가 및 적정모델 선정에 있어 적합한 방안 고찰\n",
        "*   아직 다 이해하지 못한 부분에 대한 인지 및 향후 보완사항 검토\n",
        "\n",
        "\n",
        "<br><br><br>\n"
      ],
      "metadata": {
        "id": "0cBbyPWRSO06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **II. 본 론**"
      ],
      "metadata": {
        "id": "S8GdvnPWSxWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "##**1. load_digits : 손글씨 이미지 판별**"
      ],
      "metadata": {
        "id": "aKGjl_wKXs-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "데이터 셋팅을 위해 필요한 모듈을 import하고, scikit-learn에서 해당 테이터를 로딩한다. 데이터를 변수에 저장한 후 전반적인 내용을 파악했다. \n",
        "\n",
        "총 1797개의 데이터가 각각 64(8x8)개의 값을 담고 있다.\n",
        "\n",
        "target(label)은 0부터 9까지의 숫자이며, 우리가 맞추고자 하는 바이기도 하다.\n",
        "\n",
        "DESCR 메서드를 통해 데이터셋의 상세설명을 확인하였을 때, 이 손글씨를 작성하는 데에 총 43명이 기여했다는 것을 알 수 있었다. 페이지 관계 상 이 부분은 별도로 출력하지 않는다. \n"
      ],
      "metadata": {
        "id": "ZsWosgCFLBvR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtSobM1ofz_Q",
        "outputId": "9d3ba0e1-45e7-4081-cdc7-55dde7088f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- data 형태: (1797, 64)\n",
            "- data 정보: dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n",
            "- label: [0 1 2 3 4 5 6 7 8 9]\n",
            "- feature: ['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7']\n"
          ]
        }
      ],
      "source": [
        "''' 데이터 셋팅 '''\n",
        "# 필요한 모듈 import\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# 데이터 준비\n",
        "hw = load_digits()\n",
        "\n",
        "# 데이터 이해\n",
        "hw_data = hw.data\n",
        "print(f'- data 형태: {hw_data.shape}') \n",
        "print(f'- data 정보: {hw.keys()}') \n",
        "\n",
        "hw.DESCR    # 데이터 상세정보 확인(출력생략)\n",
        "\n",
        "hw_label = hw.target\n",
        "print(f'- label: {hw.target_names}')\n",
        "print(f'- feature: {hw.feature_names}') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자료형 변환; data frame\n",
        "hw_df = pd.DataFrame(data=hw_data, columns=hw.feature_names)\n",
        "hw_df[\"label\"] = hw.target\n",
        "hw_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "-Yzldc18hUi3",
        "outputId": "c294f47c-09e6-4a8d-9857-eabb87fc81ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
              "0           0.0        0.0        5.0       13.0        9.0        1.0   \n",
              "1           0.0        0.0        0.0       12.0       13.0        5.0   \n",
              "2           0.0        0.0        0.0        4.0       15.0       12.0   \n",
              "3           0.0        0.0        7.0       15.0       13.0        1.0   \n",
              "4           0.0        0.0        0.0        1.0       11.0        0.0   \n",
              "...         ...        ...        ...        ...        ...        ...   \n",
              "1792        0.0        0.0        4.0       10.0       13.0        6.0   \n",
              "1793        0.0        0.0        6.0       16.0       13.0       11.0   \n",
              "1794        0.0        0.0        1.0       11.0       15.0        1.0   \n",
              "1795        0.0        0.0        2.0       10.0        7.0        0.0   \n",
              "1796        0.0        0.0       10.0       14.0        8.0        1.0   \n",
              "\n",
              "      pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_7  pixel_7_0  \\\n",
              "0           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
              "1           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
              "2           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
              "3           0.0        0.0        0.0        8.0  ...        0.0        0.0   \n",
              "4           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
              "...         ...        ...        ...        ...  ...        ...        ...   \n",
              "1792        0.0        0.0        0.0        1.0  ...        0.0        0.0   \n",
              "1793        1.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
              "1794        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
              "1795        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
              "1796        0.0        0.0        0.0        2.0  ...        0.0        0.0   \n",
              "\n",
              "      pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  pixel_7_6  \\\n",
              "0           0.0        6.0       13.0       10.0        0.0        0.0   \n",
              "1           0.0        0.0       11.0       16.0       10.0        0.0   \n",
              "2           0.0        0.0        3.0       11.0       16.0        9.0   \n",
              "3           0.0        7.0       13.0       13.0        9.0        0.0   \n",
              "4           0.0        0.0        2.0       16.0        4.0        0.0   \n",
              "...         ...        ...        ...        ...        ...        ...   \n",
              "1792        0.0        2.0       14.0       15.0        9.0        0.0   \n",
              "1793        0.0        6.0       16.0       14.0        6.0        0.0   \n",
              "1794        0.0        2.0        9.0       13.0        6.0        0.0   \n",
              "1795        0.0        5.0       12.0       16.0       12.0        0.0   \n",
              "1796        1.0        8.0       12.0       14.0       12.0        1.0   \n",
              "\n",
              "      pixel_7_7  label  \n",
              "0           0.0      0  \n",
              "1           0.0      1  \n",
              "2           0.0      2  \n",
              "3           0.0      3  \n",
              "4           0.0      4  \n",
              "...         ...    ...  \n",
              "1792        0.0      9  \n",
              "1793        0.0      0  \n",
              "1794        0.0      8  \n",
              "1795        0.0      9  \n",
              "1796        0.0      8  \n",
              "\n",
              "[1797 rows x 65 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90d22d31-561e-4eec-9369-d8eeb857bcfa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel_0_0</th>\n",
              "      <th>pixel_0_1</th>\n",
              "      <th>pixel_0_2</th>\n",
              "      <th>pixel_0_3</th>\n",
              "      <th>pixel_0_4</th>\n",
              "      <th>pixel_0_5</th>\n",
              "      <th>pixel_0_6</th>\n",
              "      <th>pixel_0_7</th>\n",
              "      <th>pixel_1_0</th>\n",
              "      <th>pixel_1_1</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel_6_7</th>\n",
              "      <th>pixel_7_0</th>\n",
              "      <th>pixel_7_1</th>\n",
              "      <th>pixel_7_2</th>\n",
              "      <th>pixel_7_3</th>\n",
              "      <th>pixel_7_4</th>\n",
              "      <th>pixel_7_5</th>\n",
              "      <th>pixel_7_6</th>\n",
              "      <th>pixel_7_7</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1792</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1793</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1794</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1795</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1796</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1797 rows × 65 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90d22d31-561e-4eec-9369-d8eeb857bcfa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90d22d31-561e-4eec-9369-d8eeb857bcfa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90d22d31-561e-4eec-9369-d8eeb857bcfa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "학습을 위해 데이터를 구분한다.\n",
        "\n",
        "학습에 사용될 Training Dataset과 모델의 성능을 평가할 Test Dataset으로 데이터를 나누기 위해 scikit-learn이 제공하는 **train_test_split**을 활용하였다.\n",
        "\n",
        "Test Dataset의 비율은 전체 데이터의 20%로  설정(test_size=0.2)하고, 랜덤성을 특정(random_state=42)하였다.\n",
        "\n",
        "아래와 같이 Training Dataset(X_train) 1437개, Test Dataset(X_test) 360개로 구분되었다."
      ],
      "metadata": {
        "id": "KvmmL2J8pLjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구분 (X: input, y: target)\n",
        "X_train, X_test, y_train, y_test = train_test_split(hw_data, hw_label, test_size=0.2, random_state=42)\n",
        "\n",
        "print('- X_train 개수: ', len(X_train),', X_test 개수: ', len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2WibXlfivxz",
        "outputId": "dca75f55-4659-44b4-ddf8-0ebccd731fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- X_train 개수:  1437 , X_test 개수:  360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "데이터 셋팅이 완료 되었으니 이제 학습을 시켜보자.\n",
        "\n",
        "제시된 총 다섯가지의 모델을 사용하여 학습한다.\n",
        "\n",
        "*   **Decision Tree**\n",
        "*   **Random Forest**\n",
        "*   **Support Vector Machine(SVM)**\n",
        "*   **SGD Classifier**\n",
        "*   **Logistic Regression**"
      ],
      "metadata": {
        "id": "Fnpk1I1oq91T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습01.] Decision Tree**"
      ],
      "metadata": {
        "id": "Cf1AxKs-lmrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "decision_tree = DecisionTreeClassifier(random_state=32)\n",
        "\n",
        "# 학습\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "# 예측\n",
        "y_pred_01 = decision_tree.predict(X_test)\n",
        "\n",
        "# 정확도\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred_01)\n",
        "print(f'► Decision Tree의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYPVCmTJlkjA",
        "outputId": "94a3a46e-c2bb-47c6-c2e3-cc2906f2b095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► Decision Tree의 정확도는 0.8527777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습02.] Random Forest**"
      ],
      "metadata": {
        "id": "CrIapxB_yEvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest = RandomForestClassifier(random_state=32)\n",
        "\n",
        "# 학습\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# 예측\n",
        "y_pred_02 = random_forest.predict(X_test)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test, y_pred_02)\n",
        "print(f'► Random Forest의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHB9iN_XyMnq",
        "outputId": "81235181-ac84-46a1-dfcf-1dc921ba9ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► Random Forest의 정확도는 0.9805555555555555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습03.] Support Vector Machine(SVM)**"
      ],
      "metadata": {
        "id": "QXaqDach0v6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "from sklearn import svm\n",
        "svm_model = svm.SVC(random_state=32)\n",
        "\n",
        "# 학습\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 예측\n",
        "y_pred_03 = svm_model.predict(X_test)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test, y_pred_03)\n",
        "print(f'► SVM의 정확도는 {accuracy}') \n",
        "\n",
        "''' 이건 한 번 그래프로 그려볼 수 있을까 '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UBrMqOVmyHVL",
        "outputId": "52005240-5817-46a6-8e36-f3f416d7cc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► SVM의 정확도는 0.9861111111111112\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 이건 한 번 그래프로 그려볼 수 있을까 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습04.] SGD Classifier**"
      ],
      "metadata": {
        "id": "HbUpTqQL21w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "sgd_model = SGDClassifier(random_state=32)\n",
        "\n",
        "# 학습 \n",
        "sgd_model.fit(X_train, y_train)\n",
        "\n",
        "# 예측\n",
        "y_pred_04 = sgd_model.predict(X_test)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test, y_pred_04)\n",
        "print(f'► SGD Classifier의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX0E3Ot0x9QB",
        "outputId": "2e40c7fa-654a-4466-abc5-6193e0556a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► SGD Classifier의 정확도는 0.9583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습05.] Logistic Regression**"
      ],
      "metadata": {
        "id": "qE0ZgOs-39T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_model = LogisticRegression(max_iter=4000)   # max_iter값 증가\n",
        "\n",
        "# 학습\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# 예측\n",
        "y_pred_05 = logistic_model.predict(X_test)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test, y_pred_05)\n",
        "print(f'► Logistic Regression의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGuKUwb54Dbd",
        "outputId": "0a791b19-7765-4eb7-a20e-8687243f51ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► Logistic Regression의 정확도는 0.9722222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "            \n",
        "<br>\n",
        "\n",
        "###**모델 평가**"
      ],
      "metadata": {
        "id": "pBFp8ujD7_5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정확도(accuracy)를 비교할 경우,\n",
        "**Support Vector Machine(SVM)**이 정확도 **98.6%**로 가장 적합한 모델로 보인다.  \n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "정확도를 보완할 지표로 **F1 score** 값을 추가로 살펴보자.\n",
        "\n",
        "0부터 9까지의 손글씨를 분류하는 행위는 해당 숫자를 Negative로 판별하여 놓치는 경우(FN)와 다른 숫자를 해당 숫자로 오인하여 Positive로 판별하는 경우(FP) 둘 다 치명적인 오류가 될 수 있다고 본다. \n",
        "\n",
        "- *예를 들면, 손글씨로 쓴 전화번호를 인식한다고 쳤을 때 FN, FP의 경우 둘 다 우리는 연락할 방법이 없어진다.*\n",
        "\n",
        "따라서 Recall과 Precision의 조화평균인 F1 score를 선택한다.\n",
        "\n",
        "다음은 Confusion matrix에 의해 각 모델의 성능지표 값을 나타낸 report이다. label별 **F1 score**가 비교적 고르게 나온 것을 알 수 있다. 데이터 밸런스가 좋다고 판단된다. 따라서 앞서 살펴 본 정확도(accuracy)를 지표로 하여 **Support Vector Machine(SVM)**을 채택한다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_0ygT-t19Jw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Decision Tree report\n",
        "confusion_matrix(y_test, y_pred_01)\n",
        "print(classification_report(y_test, y_pred_01))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_98YeuYn8y5V",
        "outputId": "ca0456d8-6416-4e88-ca47-2c8ebbd761b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.88      0.92        33\n",
            "           1       0.81      0.79      0.80        28\n",
            "           2       0.87      0.82      0.84        33\n",
            "           3       0.79      0.91      0.85        34\n",
            "           4       0.81      0.85      0.83        46\n",
            "           5       0.91      0.83      0.87        47\n",
            "           6       0.94      0.94      0.94        35\n",
            "           7       0.81      0.85      0.83        34\n",
            "           8       0.85      0.77      0.81        30\n",
            "           9       0.80      0.88      0.83        40\n",
            "\n",
            "    accuracy                           0.85       360\n",
            "   macro avg       0.86      0.85      0.85       360\n",
            "weighted avg       0.86      0.85      0.85       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest report\n",
        "confusion_matrix(y_test, y_pred_02)\n",
        "print(classification_report(y_test, y_pred_02))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237823ff-7880-428d-8e7b-095e9917e499",
        "id": "71VJl93cAECh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        33\n",
            "           1       0.93      1.00      0.97        28\n",
            "           2       1.00      1.00      1.00        33\n",
            "           3       1.00      1.00      1.00        34\n",
            "           4       1.00      1.00      1.00        46\n",
            "           5       0.96      0.98      0.97        47\n",
            "           6       0.97      0.97      0.97        35\n",
            "           7       0.97      0.97      0.97        34\n",
            "           8       1.00      0.93      0.97        30\n",
            "           9       0.97      0.95      0.96        40\n",
            "\n",
            "    accuracy                           0.98       360\n",
            "   macro avg       0.98      0.98      0.98       360\n",
            "weighted avg       0.98      0.98      0.98       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM report\n",
        "confusion_matrix(y_test, y_pred_03)\n",
        "print(classification_report(y_test, y_pred_03))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c69b7f7-64a5-40d6-9d47-f0e1e2a6d460",
        "id": "iplhlTrIAFhJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        33\n",
            "           1       1.00      1.00      1.00        28\n",
            "           2       1.00      1.00      1.00        33\n",
            "           3       1.00      1.00      1.00        34\n",
            "           4       1.00      1.00      1.00        46\n",
            "           5       0.98      0.98      0.98        47\n",
            "           6       0.97      1.00      0.99        35\n",
            "           7       0.97      0.97      0.97        34\n",
            "           8       1.00      0.97      0.98        30\n",
            "           9       0.95      0.95      0.95        40\n",
            "\n",
            "    accuracy                           0.99       360\n",
            "   macro avg       0.99      0.99      0.99       360\n",
            "weighted avg       0.99      0.99      0.99       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Classifier report\n",
        "confusion_matrix(y_test, y_pred_04)\n",
        "print(classification_report(y_test, y_pred_04))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b592888-27d2-4403-b964-3a87550a2b6f",
        "id": "VNO4aF2eAF-l"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        33\n",
            "           1       0.93      0.96      0.95        28\n",
            "           2       0.94      1.00      0.97        33\n",
            "           3       0.94      0.97      0.96        34\n",
            "           4       1.00      0.98      0.99        46\n",
            "           5       0.94      0.96      0.95        47\n",
            "           6       0.94      0.97      0.96        35\n",
            "           7       1.00      0.97      0.99        34\n",
            "           8       0.90      0.90      0.90        30\n",
            "           9       0.97      0.88      0.92        40\n",
            "\n",
            "    accuracy                           0.96       360\n",
            "   macro avg       0.96      0.96      0.96       360\n",
            "weighted avg       0.96      0.96      0.96       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression report\n",
        "confusion_matrix(y_test, y_pred_05)\n",
        "print(classification_report(y_test, y_pred_05))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec3c237-1078-45d0-b3de-e30cda511602",
        "id": "7nOyoIhJAGjH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        33\n",
            "           1       0.97      1.00      0.98        28\n",
            "           2       0.97      1.00      0.99        33\n",
            "           3       0.97      0.97      0.97        34\n",
            "           4       1.00      0.98      0.99        46\n",
            "           5       0.92      0.94      0.93        47\n",
            "           6       0.97      0.97      0.97        35\n",
            "           7       1.00      0.97      0.99        34\n",
            "           8       0.97      0.97      0.97        30\n",
            "           9       0.97      0.95      0.96        40\n",
            "\n",
            "    accuracy                           0.97       360\n",
            "   macro avg       0.97      0.97      0.97       360\n",
            "weighted avg       0.97      0.97      0.97       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "\n",
        "###**소 결**\n",
        "\n"
      ],
      "metadata": {
        "id": "gvqOOHI3IgXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5가지 모델을 통해 손글씨 이미지를 판별한 결과는 다음과 같다.\n",
        "\n",
        "*   **Decision Tree**\n",
        "    *   accuracy : 0.85 \n",
        "    *   f1-score : (min)0.80,  (max)0.94\n",
        "*   **Random Forest**\n",
        "    *   accuracy : 0.98\n",
        "    *   f1-score : (min)0.96,  (max)1.00\n",
        "*   **Support Vector Machine(SVM)**\n",
        "    *   accuracy : 0.99\n",
        "    *   f1-score : (min)0.95,  (max)1.00\n",
        "*   **SGD Classifier**\n",
        "    *   accuracy : 0.96\n",
        "    *   f1-score : (min)0.90,  (max)1.00\n",
        "*   **Logistic Regression**\n",
        "    *   accuracy : 0.97\n",
        "    *   f1-score : (min)0.93,  (max)1.00\n",
        "\n",
        "<br>\n",
        "\n",
        "손글씨 분류는 해당 숫자를 Negative로 판별하여 놓치는 경우(FN)와 다른 숫자를 해당 숫자로 오인하여 Positive로 판별하는 경우(FP) 둘 다 치명적인 오류가 될 수 있다고 판단하여 F1 score를 추가 평가지표로 선택했다.\n",
        "\n",
        "정확도(accuracy) 및 F1-score를 기준으로 하여 **Support Vector Machine(SVM)**을 가장 적합한 모델로 채택한다.\n",
        "\n",
        "\n",
        "<br><br><br><br><br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "wpl4EOZlKRVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "##**2. load_wine : 와인 종류 판별**"
      ],
      "metadata": {
        "id": "JyGc8569rtJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-\n",
        "\n",
        "데이터 셋팅을 위해 scikit-learn에서 해당 테이터를 로딩한다. 데이터를 변수에 저장한 후 전반적인 내용을 파악했다.\n",
        "\n",
        "데이터는 총 178개로 그 양이 많지 않다. 각각의 데이터는 13개의 값을 담고 있다.\n",
        "\n",
        "target(label)은 'class_0' 'class_1' 'class_2'으로 3가지 종류의 와인을 의미한다.\n",
        "\n",
        "DESCR 메서드를 통해 데이터셋의 상세설명을 확인하면 해당 데이터 상의 와인은 모두 이탈리아 품종으로, 같은 지역에서 서로 다른 경작자에 의해 재배된 것임을 알 수 있다. 페이지 관계 상 이 부분은 별도로 출력하지 않는다. \n"
      ],
      "metadata": {
        "id": "Kh2j1serQx2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275c1e95-df30-4532-deac-a167bc10145b",
        "id": "26yC3nkvsFbW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- data 형태: (178, 13)\n",
            "- data 정보: dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])\n",
            "- label: ['class_0' 'class_1' 'class_2']\n",
            "- feature: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
          ]
        }
      ],
      "source": [
        "''' 데이터 셋팅 '''\n",
        "# 필요한 모듈 import(위에서 이미 import한 모듈은 생략)\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# 데이터 준비\n",
        "wt = load_wine()\n",
        "\n",
        "# 데이터 이해\n",
        "wt_data = wt.data\n",
        "print(f'- data 형태: {wt_data.shape}') \n",
        "print(f'- data 정보: {wt.keys()}')\n",
        "\n",
        "wt.DESCR    # 데이터 상세정보 확인(출력생략)\n",
        "\n",
        "wt_label = wt.target\n",
        "print(f'- label: {wt.target_names}')\n",
        "print(f'- feature: {wt.feature_names}') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자료형 변환; data frame\n",
        "wt_df = pd.DataFrame(data=wt_data, columns=wt.feature_names)\n",
        "wt_df[\"label\"] = wt.target\n",
        "wt_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "38e40d73-2dcf-4865-e31e-2aa101411af5",
        "id": "bqHaEmovsFbX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
              "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
              "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
              "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
              "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
              "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
              "..       ...         ...   ...                ...        ...            ...   \n",
              "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
              "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
              "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
              "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
              "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
              "\n",
              "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
              "0          3.06                  0.28             2.29             5.64  1.04   \n",
              "1          2.76                  0.26             1.28             4.38  1.05   \n",
              "2          3.24                  0.30             2.81             5.68  1.03   \n",
              "3          3.49                  0.24             2.18             7.80  0.86   \n",
              "4          2.69                  0.39             1.82             4.32  1.04   \n",
              "..          ...                   ...              ...              ...   ...   \n",
              "173        0.61                  0.52             1.06             7.70  0.64   \n",
              "174        0.75                  0.43             1.41             7.30  0.70   \n",
              "175        0.69                  0.43             1.35            10.20  0.59   \n",
              "176        0.68                  0.53             1.46             9.30  0.60   \n",
              "177        0.76                  0.56             1.35             9.20  0.61   \n",
              "\n",
              "     od280/od315_of_diluted_wines  proline  label  \n",
              "0                            3.92   1065.0      0  \n",
              "1                            3.40   1050.0      0  \n",
              "2                            3.17   1185.0      0  \n",
              "3                            3.45   1480.0      0  \n",
              "4                            2.93    735.0      0  \n",
              "..                            ...      ...    ...  \n",
              "173                          1.74    740.0      2  \n",
              "174                          1.56    750.0      2  \n",
              "175                          1.56    835.0      2  \n",
              "176                          1.62    840.0      2  \n",
              "177                          1.60    560.0      2  \n",
              "\n",
              "[178 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78b9e771-0033-451d-a5d2-cfa63fd8297a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.30</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.20</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96.0</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78b9e771-0033-451d-a5d2-cfa63fd8297a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78b9e771-0033-451d-a5d2-cfa63fd8297a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78b9e771-0033-451d-a5d2-cfa63fd8297a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "학습을 위해 데이터를 구분한다.\n",
        "\n",
        "학습에 사용될 Training Dataset과 모델의 성능을 평가할 Test Dataset으로 데이터를 나누기 위해 scikit-learn이 제공하는 train_test_split을 활용하였다.\n",
        "\n",
        "Test Dataset의 비율은 전체 데이터의 20%로 설정(test_size=0.2)하고, 랜덤성을 특정(random_state=42)했다.\n",
        "\n",
        "아래와 같이 Training Dataset(X_train2) 142개, Test Dataset(X_test2) 36개로 구분되었다."
      ],
      "metadata": {
        "id": "5e18SLlYKiVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구분\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(wt_data, wt_label, test_size=0.2, random_state=42)\n",
        "\n",
        "print('- X_train2 개수: ', len(X_train2),',  X_test2 개수: ', len(X_test2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ffbd062-8ebd-4c13-ed21-06a18fa221a5",
        "id": "FEkCsqO-sFbY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- X_train2 개수:  142 ,  X_test2 개수:  36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "데이터 셋팅을 완료했으니 이제 학습할 차례다.\n",
        "\n",
        "<br>\n",
        "\n",
        "###**[학습01.] Decision Tree**"
      ],
      "metadata": {
        "id": "1jAAuVahXFKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "# 학습\n",
        "decision_tree.fit(X_train2, y_train2)\n",
        "\n",
        "# 예측\n",
        "y_pred_11 = decision_tree.predict(X_test2)\n",
        "\n",
        "# 정확도\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test2, y_pred_11)\n",
        "print(f'► Decision Tree의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b81d6c2-e2a3-4565-cf38-274ef67017bf",
        "id": "z_HU-v4rXFKZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► Decision Tree의 정확도는 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습02.] Random Forest**"
      ],
      "metadata": {
        "id": "6FpcyjKoXFKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "# 학습\n",
        "random_forest.fit(X_train2, y_train2)\n",
        "\n",
        "# 예측\n",
        "y_pred_12 = random_forest.predict(X_test2)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test2, y_pred_12)\n",
        "print(f'► Random Forest의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbabf596-26c3-4e90-844a-eee56d1e80ae",
        "id": "ymWsg_juXFKa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► Random Forest의 정확도는 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습03.] Support Vector Machine(SVM)**"
      ],
      "metadata": {
        "id": "HsBRWZIbXFKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "# 학습\n",
        "svm_model.fit(X_train2, y_train2)\n",
        "\n",
        "# 예측\n",
        "y_pred_13 = svm_model.predict(X_test2)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test2, y_pred_13)\n",
        "print(f'► SVM의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60fe2efe-d6cb-4406-dd5e-dd6544ec9c83",
        "id": "HL7lY_F9XFKb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► SVM의 정확도는 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습04.] SGD Classifier**"
      ],
      "metadata": {
        "id": "OupzL9PRXFKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "# 학습 \n",
        "sgd_model.fit(X_train2, y_train2)\n",
        "\n",
        "# 예측\n",
        "y_pred_14 = sgd_model.predict(X_test2)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test2, y_pred_14)\n",
        "print(f'► SGD Classifier의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9aa988-56cf-4fac-f813-5231a4e6f5cd",
        "id": "zeTU80jdXFKc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► SGD Classifier의 정확도는 0.5833333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습05.] Logistic Regression**"
      ],
      "metadata": {
        "id": "lqcHeLE8XFKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "# 학습\n",
        "logistic_model.fit(X_train2, y_train2)\n",
        "\n",
        "# 예측\n",
        "y_pred_15 = logistic_model.predict(X_test2)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test2, y_pred_15)\n",
        "print(f'► Logistic Regression의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "id": "rn6j0hRpXFKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0511c7-d79c-43c5-c470-069dc6c822ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► Logistic Regression의 정확도는 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "            \n",
        "<br>\n",
        "\n",
        "###**모델 평가**"
      ],
      "metadata": {
        "id": "Qj4DHe3EX7OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정확도(accuracy)를 비교할 경우,\n",
        "**Random Forest**와 **Logistic Regression**의 정확도가 **100.0%**로 나타났다.\n",
        "\n",
        "적정성을 의심 해볼 필요가 있는 수치라고 생각된다. 일단 전체 데이터 갯수(178개)가 너무 적은 것이 문제일 수 있겠다. 물론 Random Forest의 경우, Decision Tree를 보완한 대표적 앙상블 학습 알고리즘으로 Decision Tree(94.4%)에 비해 성능이 향상된 측면도 있을 것이다.\n",
        "\n",
        "좀 더 자세한 파악을 위해 오차행렬(confusion_matrix)을 이용한 classification report를 살펴보자.\n",
        "\n",
        "아래의 report를 살펴보면, 우선 SVM과 SGD Classifier의 F1-score가 와인 유형별로 매우 차이 남을 알 수 있다. 반면, Decision Tree의 경우 F1-score가 비교적 고르게 나타나며 Accuracy와도 유사한 값을 보인다. Random Forest와 Logistic Regression 모델은 report 상에서도 모든 값이 1.0으로 나왔다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hJRHla8cX7OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Decision Tree report\n",
        "confusion_matrix(y_test2, y_pred_11)\n",
        "print(classification_report(y_test2, y_pred_11))"
      ],
      "metadata": {
        "id": "NVL5lL2xX7OZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029f3c2d-a65c-441a-889e-5514f3412cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93        14\n",
            "           1       0.93      1.00      0.97        14\n",
            "           2       1.00      0.88      0.93         8\n",
            "\n",
            "    accuracy                           0.94        36\n",
            "   macro avg       0.95      0.93      0.94        36\n",
            "weighted avg       0.95      0.94      0.94        36\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest report\n",
        "confusion_matrix(y_test2, y_pred_12)\n",
        "print(classification_report(y_test2, y_pred_12))"
      ],
      "metadata": {
        "id": "5FzFQ39vX7OZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bdfa21b-1ff9-4490-a807-db649540ba8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       1.00      1.00      1.00        14\n",
            "           2       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        36\n",
            "   macro avg       1.00      1.00      1.00        36\n",
            "weighted avg       1.00      1.00      1.00        36\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM report\n",
        "confusion_matrix(y_test2, y_pred_13)\n",
        "print(classification_report(y_test2, y_pred_13))"
      ],
      "metadata": {
        "id": "o0YQSPPDX7Oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118a0038-a23a-439b-d0a9-969b49a43feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       0.73      0.79      0.76        14\n",
            "           2       0.57      0.50      0.53         8\n",
            "\n",
            "    accuracy                           0.81        36\n",
            "   macro avg       0.77      0.76      0.76        36\n",
            "weighted avg       0.80      0.81      0.80        36\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Classifier report\n",
        "confusion_matrix(y_test2, y_pred_14)\n",
        "print(classification_report(y_test2, y_pred_14, zero_division=1))"
      ],
      "metadata": {
        "id": "ZHM3vywRX7Oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b222eb6d-7b8c-4493-cfe4-122734af3264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.71      0.83        14\n",
            "           1       1.00      0.21      0.35        14\n",
            "           2       0.35      1.00      0.52         8\n",
            "\n",
            "    accuracy                           0.58        36\n",
            "   macro avg       0.78      0.64      0.57        36\n",
            "weighted avg       0.86      0.58      0.58        36\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression report\n",
        "confusion_matrix(y_test2, y_pred_15)\n",
        "print(classification_report(y_test2, y_pred_15))"
      ],
      "metadata": {
        "id": "DpzMfvQJX7Oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d0a943-ac3a-4f3a-bf7a-8716d39679bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       1.00      1.00      1.00        14\n",
            "           2       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        36\n",
            "   macro avg       1.00      1.00      1.00        36\n",
            "weighted avg       1.00      1.00      1.00        36\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "모델이 적합한 성능을 지녔는지 알아보기 위해 추가적으로 훈련 데이터와 테스트 데이터의 검증점수를 비교해보자. 모델의 과적합 여부를 파악할 수 있다.\n",
        "\n",
        "아래에서 값을 확인하면, Decision Tree의 경우 훈련 데이터의 점수에 비해 테스트 데이터의 점수가 약간 낮게 나왔고 그 차이가 그리 크지 않음을 알 수 있다. Random Forest는 테스트 점수가 더 향상되어 테스트 점수도 만점이 나왔다. \n",
        "\n",
        "Logistic Regression을 보면 훈련 데이터 점수에 비해 테스트 데이터 점수가 더 높은 과소적합(underfitting)임을 알 수 있다. 전체 데이터의 수량 자체가 근본적인 문제라고 생각되나 성능을 개선할 수 있는 방법은 없을지 찾아보고자 한다."
      ],
      "metadata": {
        "id": "0W8oUA_ZU4oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 문제점 분석 '''\n",
        "# Decision Tree\n",
        "print('- 훈련 데이터 검증점수:', decision_tree.score(X_train2, y_train2))\n",
        "print('- 테스트 데이터 검증점수:', decision_tree.score(X_test2, y_test2))\n"
      ],
      "metadata": {
        "id": "AcWMjesO_e6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d14b69-3861-4553-d058-91d20c59b0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 훈련 데이터 검증점수: 1.0\n",
            "- 테스트 데이터 검증점수: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "print('- 훈련 데이터 검증점수:', random_forest.score(X_train2, y_train2))\n",
        "print('- 테스트 데이터 검증점수:', random_forest.score(X_test2, y_test2))\n"
      ],
      "metadata": {
        "id": "IJpISLGZ-SG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2375a75c-3b70-423d-e25a-5742299f31ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 훈련 데이터 검증점수: 1.0\n",
            "- 테스트 데이터 검증점수: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression: 과소적합\n",
        "print('- 훈련 데이터 검증점수:', logistic_model.score(X_train2, y_train2))\n",
        "print('- 테스트 데이터 검증점수:', logistic_model.score(X_test2, y_test2))\n"
      ],
      "metadata": {
        "id": "dsOVX2Q3kcsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec6dbf5-b657-4640-b1bf-1ddc4412e0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 훈련 데이터 검증점수: 0.9929577464788732\n",
            "- 테스트 데이터 검증점수: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "\n",
        "먼저 Random Forest와는 다른 앙상블 모델을 활용하고 그 결과를 비교해 보자.\n",
        "\n",
        "깊이가 얕은 결정트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 **그레이디언트 부스팅**과 노드를 분할할 때 최적의 분할을 빠르게 찾을 수 있게 해주는 **히스토그램 기반 그레이티언트 부스팅**을 사용한다.\n",
        "\n",
        "과적합을 억제하면서 높은 성능을 낼 수 있는 방안이 되기를 기대한다.\n"
      ],
      "metadata": {
        "id": "jg1Mz4lFNnP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 그레이디언트 부스팅\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import numpy as np\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "scores01 = cross_validate(gb, X_train2, y_train2, \n",
        "                       return_train_score=True, n_jobs=-1)    # 교차검증 수행, 검증점수 및 훈련세트 점수도 반환\n",
        "print('- 훈련 데이터 검증점수:', np.mean(scores01['train_score']), ',  테스트 데이터 검증점수:',np.mean(scores01['test_score']))"
      ],
      "metadata": {
        "id": "RvTXZSPG_WRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a53d775-e403-4f6a-c99a-c5b63186015c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 훈련 데이터 검증점수: 1.0 ,  테스트 데이터 검증점수: 0.922167487684729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그레이디언트 부스팅(결정트리 갯수 및 학습률 조정)\n",
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "scores02 = cross_validate(gb, X_train2, y_train2, \n",
        "                       return_train_score=True, n_jobs=-1)\n",
        "print('- 훈련 데이터 검증점수:', np.mean(scores02['train_score']), ',  테스트 데이터 검증점수:', np.mean(scores02['test_score']))"
      ],
      "metadata": {
        "id": "nJJaHkN_EczX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c27ce274-4564-4899-b38f-39e302a36535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 훈련 데이터 검증점수: 1.0 ,  테스트 데이터 검증점수: 0.9435960591133006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 기반 그레이디언트 부스팅\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores03 = cross_validate(hgb, X_train2, y_train2, return_train_score=True)\n",
        "print('- 훈련 데이터 검증점수:', np.mean(scores03['train_score']), ',  테스트 데이터 검증점수:', np.mean(scores03['test_score']))"
      ],
      "metadata": {
        "id": "PMG4tMCoDHwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f31eba3a-5c13-49ca-e366-0d64c44af5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 훈련 데이터 검증점수: 1.0 ,  테스트 데이터 검증점수: 0.9576354679802955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "그레이디언트 부스팅의 결과를 보면, 테스트 데이터의 검증점수를 적정하게 낮추는 데에는 성공했지만 훈련 데이터의 검증점수는 여전히 1.0로 둘 사이의 갭은 커졌다.\n",
        "\n",
        "그레이디언트 부스팅의 결정트리 갯수를 높이고(500), 학습률을 조정(0.2)해서 테스트 검증점수를 조금 더 높일 수 있었다.\n",
        "\n",
        "그리고 그레이디언트 부스팅보다 조금 더 높은 성능을 기대할 수 있는 히스토그램 기반 그레이디언트 부스팅을 활용하여 훈련 데이터 검증점수와 테스트 데이터 검증점수 간의 차이를 더 좁혔다.\n",
        "\n",
        "다만, 훈련 데이터 검증점수 자체가 지속적으로 1.0로 나오는 부분이 적합한지에 관해서는 향후 추가적인 연구가 필요하다고 생각한다. \n",
        "\n",
        "-\n",
        "\n",
        "이번엔 Logistic Regression 모델의 과소적합 문제를 살펴보자. \n",
        "\n",
        "훈련 데이터 점수(0.99)에 비해 테스트 데이터 점수(1.00)가 더 높으며, 모델이 단순하여 훈련이 적절히 되지 않았을 가능성이 있었다.\n",
        "\n",
        "규제를 통해 훈련정도를 조정할 수 있을지 확인해 보도록 한다. 계수의 제곱(R^2)을 기준으로 규제를 적용하는 **Ridge** 방식을 적용한다."
      ],
      "metadata": {
        "id": "522Riix7Qxkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Regularization '''\n",
        "# 표준점수로 변환\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "ss.fit(X_train2)\n",
        "train_scaled = ss.transform(X_train2)\n",
        "test_scaled = ss.transform(X_test2)\n",
        "\n",
        "# 릿지 규제 적용\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge()\n",
        "ridge.fit(train_scaled, y_train2)\n",
        "print(ridge.score(train_scaled, y_train2))\n",
        "print(ridge.score(test_scaled, y_test2))"
      ],
      "metadata": {
        "id": "xTNZANHC8GrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f780a8-6456-42f7-c2a4-1f9106cc99b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9028583074553864\n",
            "0.8822517421818129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "릿지 규제를 적용함으로써 테스트 세트의 점수가 낮아졌지만 훈련 세트의 점수를 포함하여 전체적으로 점수가 낮아져 성능이 개선되었다고 보기는 어렵다. \n",
        "\n",
        "하이퍼파라미터를 조정하여 적합한 규제 정도를 찾아내기 위해 alpha 값에 대한 R^2의 그래프를 그려보았다. 아래 그래프에서 보는 것과 같이 alpha 값을 조정한다고 해서 특별히 점수가 높아지는 지점은 없다. alpha=1을 넘으면서 점수의 하락폭이 증가하고, alpha=10을 넘으면 더 급격히 떨어짐을 알 수 있다. 훈련 세트와 데이터 세트 간의 간격은 좁아지지만 전체 점수가 하락하는 것이다. 반대로 그래프 상 최고점에 해당하는 alpha 값(0.001)을 적용했을 때도 점수의 향상률은 미미하다.\n",
        "\n",
        "일반적으로 릿지가 과대적합을 규제하는데 적합하나 해당 문제의 과소적합을 조정하는 데에는 뚜렷한 역할을 하지 못한 것으로 보인다. Logistic Regression 모델의 과소적합을 해결하는 방법에 대해서는 향후 추가적인 고찰이 필요하겠다."
      ],
      "metadata": {
        "id": "3Vmt-mQlEQic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 조정\n",
        "import matplotlib.pyplot as plt\n",
        "train_score = []\n",
        "test_score = []\n",
        "\n",
        "alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "for alpha in alpha_list:\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(train_scaled, y_train2)\n",
        "    train_score.append(ridge.score(train_scaled, y_train2))\n",
        "    test_score.append(ridge.score(test_scaled, y_test2))\n",
        "\n",
        "plt.plot(np.log10(alpha_list), train_score, label='train score')\n",
        "plt.plot(np.log10(alpha_list), test_score, label='test score')\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('R^2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UXsbcS8N970A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "9e603713-ee5e-409d-9e4f-f83918bce7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최고점에서의 값 확인(위에서 구한 초기값 적용과 비교: 대동소이)\n",
        "ridge = Ridge(alpha=0.001)\n",
        "ridge.fit(train_scaled, y_train2)\n",
        "print(ridge.score(train_scaled, y_train2))\n",
        "print(ridge.score(test_scaled, y_test2))"
      ],
      "metadata": {
        "id": "P7miHhuH_3Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896f3a1b-153c-4bb2-f2b2-c0bd137757f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9029347422087665\n",
            "0.8825137739188348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "\n",
        "###**소 결**\n",
        "\n"
      ],
      "metadata": {
        "id": "_mHhcg2nX7Oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5가지 모델을 통해 와인종류를 판별한 결과는 다음과 같다.\n",
        "\n",
        "*   **Decision Tree**\n",
        "    *   accuracy : 0.94 \n",
        "    *   f1-score : (min)0.93,  (max)0.97\n",
        "*   **Random Forest**\n",
        "    *   accuracy : 1.00\n",
        "    *   f1-score : (min)1.00,  (max)1.00\n",
        "*   **Support Vector Machine(SVM)**\n",
        "    *   accuracy : 0.81\n",
        "    *   f1-score : (min)0.53,  (max)1.00\n",
        "*   **SGD Classifier**\n",
        "    *   accuracy : 0.58\n",
        "    *   f1-score : (min)0.35,  (max)0.83\n",
        "*   **Logistic Regression**\n",
        "    *   accuracy : 1.00\n",
        "    *   f1-score : (min)1.00,  (max)1.00\n",
        "\n",
        "<br>\n",
        "\n",
        "와인 판별의 경우 재현율(Recall)과 정밀도(Precision)를 고르게 반영하는 것이 적합하다. 따라서 정확도(accuracy)와 함께 F1 score를 추가적인 평가지표로 선택하여 검토한다.\n",
        "\n",
        "정확도가 높은 모델을 중심으로 과적합 여부를 추가적으로 살펴본 결과, Logistic Regression이 과소적합 된것으로 나타났다. 규제적용(Ridge) 시 전체적인 점수가 하향되는 문제가 발생하여 적합한 모델이 아닌 것으로 판단하였다.\n",
        "\n",
        "Random Forest의 경우, 모든 측면에서 점수가 높게(1.00) 측정되어 자체적으로는 성과의 적정성을 판단하기 어려웠다. 추가로 다른 앙상블 모델을 학습시켜 결과를 비교해 보았다. 히스토그램 기반 그레이디언트 부스팅의 학습 점수(1.00)와 테스트 점수(0.96)이 균형감 있게 나왔다. 따라서 기본 모델 중에서는 **랜덤 포레스트**를 선택하고, 대안모델로 **히스토그램 기반 그레이디언트 부스팅**도 적합하다고 결론 내린다.\n",
        "\n",
        "\n",
        "\n",
        "<br><br><br><br><br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "JjB_JiTrX7Ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "##**3. load_breast_cancer: 유방암 여부 진단**"
      ],
      "metadata": {
        "id": "qzjb55U6bnHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "-\n",
        "\n",
        "데이터 셋팅을 위해 scikit-learn에서 해당 테이터를 로딩한다. 데이터를 변수에 저장한 후 전반적인 내용을 파악했다.\n",
        "\n",
        "총 569개의 데이터가 각각 30개의 값을 담고 있다.\n",
        "\n",
        "target(label)은 악성(malignant)과 양성(benign)으로 구분된다.\n",
        "\n",
        "DESCR 메서드로 확인한 결과, 해당 데이터는 유방에서 미세바늘흡인(FNA)을 통해 디지털화 된 이미지로부터 추출된 정보임을 알 수 있었다. 페이지 관계 상 이 부분은 별도로 출력하지 않는다.\n",
        " \n"
      ],
      "metadata": {
        "id": "BIkO68DWbnHc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr923aTPbnHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f60537-e188-4a8e-ca01-563ab248eca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- data 형태: (569, 30)\n",
            "- data 정보: dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
            "- label: ['malignant' 'benign']\n",
            "- feature: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
          ]
        }
      ],
      "source": [
        "''' 데이터 셋팅 '''\n",
        "# 필요한 모듈 import(위에서 이미 import한 모듈은 생략)\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# 데이터 준비\n",
        "bc = load_breast_cancer()\n",
        "\n",
        "# 데이터 이해\n",
        "bc_data = bc.data\n",
        "print(f'- data 형태: {bc_data.shape}') \n",
        "print(f'- data 정보: {bc.keys()}') \n",
        "\n",
        "bc.DESCR    # 데이터 상세정보 확인(출력생략)\n",
        "\n",
        "bc_label = bc.target\n",
        "print(f'- label: {bc.target_names}')\n",
        "print(f'- feature: {bc.feature_names}') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자료형 변환; data frame\n",
        "bc_df = pd.DataFrame(data=bc_data, columns=bc.feature_names)\n",
        "bc_df[\"label\"] = bc.target\n",
        "bc_df"
      ],
      "metadata": {
        "id": "EILfICWAbnHd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "b12fd5f2-4b7f-4337-8b3b-e1bd383ca361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0          17.99         10.38          122.80     1001.0          0.11840   \n",
              "1          20.57         17.77          132.90     1326.0          0.08474   \n",
              "2          19.69         21.25          130.00     1203.0          0.10960   \n",
              "3          11.42         20.38           77.58      386.1          0.14250   \n",
              "4          20.29         14.34          135.10     1297.0          0.10030   \n",
              "..           ...           ...             ...        ...              ...   \n",
              "564        21.56         22.39          142.00     1479.0          0.11100   \n",
              "565        20.13         28.25          131.20     1261.0          0.09780   \n",
              "566        16.60         28.08          108.30      858.1          0.08455   \n",
              "567        20.60         29.33          140.10     1265.0          0.11780   \n",
              "568         7.76         24.54           47.92      181.0          0.05263   \n",
              "\n",
              "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0             0.27760         0.30010              0.14710         0.2419   \n",
              "1             0.07864         0.08690              0.07017         0.1812   \n",
              "2             0.15990         0.19740              0.12790         0.2069   \n",
              "3             0.28390         0.24140              0.10520         0.2597   \n",
              "4             0.13280         0.19800              0.10430         0.1809   \n",
              "..                ...             ...                  ...            ...   \n",
              "564           0.11590         0.24390              0.13890         0.1726   \n",
              "565           0.10340         0.14400              0.09791         0.1752   \n",
              "566           0.10230         0.09251              0.05302         0.1590   \n",
              "567           0.27700         0.35140              0.15200         0.2397   \n",
              "568           0.04362         0.00000              0.00000         0.1587   \n",
              "\n",
              "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
              "0                   0.07871  ...          17.33           184.60      2019.0   \n",
              "1                   0.05667  ...          23.41           158.80      1956.0   \n",
              "2                   0.05999  ...          25.53           152.50      1709.0   \n",
              "3                   0.09744  ...          26.50            98.87       567.7   \n",
              "4                   0.05883  ...          16.67           152.20      1575.0   \n",
              "..                      ...  ...            ...              ...         ...   \n",
              "564                 0.05623  ...          26.40           166.10      2027.0   \n",
              "565                 0.05533  ...          38.25           155.00      1731.0   \n",
              "566                 0.05648  ...          34.12           126.70      1124.0   \n",
              "567                 0.07016  ...          39.42           184.60      1821.0   \n",
              "568                 0.05884  ...          30.37            59.16       268.6   \n",
              "\n",
              "     worst smoothness  worst compactness  worst concavity  \\\n",
              "0             0.16220            0.66560           0.7119   \n",
              "1             0.12380            0.18660           0.2416   \n",
              "2             0.14440            0.42450           0.4504   \n",
              "3             0.20980            0.86630           0.6869   \n",
              "4             0.13740            0.20500           0.4000   \n",
              "..                ...                ...              ...   \n",
              "564           0.14100            0.21130           0.4107   \n",
              "565           0.11660            0.19220           0.3215   \n",
              "566           0.11390            0.30940           0.3403   \n",
              "567           0.16500            0.86810           0.9387   \n",
              "568           0.08996            0.06444           0.0000   \n",
              "\n",
              "     worst concave points  worst symmetry  worst fractal dimension  label  \n",
              "0                  0.2654          0.4601                  0.11890      0  \n",
              "1                  0.1860          0.2750                  0.08902      0  \n",
              "2                  0.2430          0.3613                  0.08758      0  \n",
              "3                  0.2575          0.6638                  0.17300      0  \n",
              "4                  0.1625          0.2364                  0.07678      0  \n",
              "..                    ...             ...                      ...    ...  \n",
              "564                0.2216          0.2060                  0.07115      0  \n",
              "565                0.1628          0.2572                  0.06637      0  \n",
              "566                0.1418          0.2218                  0.07820      0  \n",
              "567                0.2650          0.4087                  0.12400      0  \n",
              "568                0.0000          0.2871                  0.07039      1  \n",
              "\n",
              "[569 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8bfd5309-cf04-4284-91b7-7ce340276926\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>...</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>...</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>...</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>...</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>...</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8bfd5309-cf04-4284-91b7-7ce340276926')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8bfd5309-cf04-4284-91b7-7ce340276926 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8bfd5309-cf04-4284-91b7-7ce340276926');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "학습을 위해 데이터를 구분한다.\n",
        "\n",
        "학습에 사용될 Training Dataset과 모델의 성능을 평가할 Test Dataset으로 데이터를 나누기 위해 scikit-learn이 제공하는 train_test_split을 활용하였다.\n",
        "\n",
        "Test Dataset의 비율은 전체 데이터의 20%로 설정(test_size=0.2)하고, 랜덤성을 특정(random_state=42)하였다.\n",
        "\n",
        "아래와 같이 Training Dataset(X_train3) 455개, Test Dataset(X_test3) 114개로 구분되었다."
      ],
      "metadata": {
        "id": "NidC8ZmIKQAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구분 \n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(bc_data, bc_label, test_size=0.2, random_state=42)\n",
        "\n",
        "print('- X_train3 개수: ', len(X_train3),', X_test3 개수: ', len(X_test3))\n"
      ],
      "metadata": {
        "id": "NoJ1JYiEbnHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c211ee-f94e-48e3-8e87-9fe0bbe3fd65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- X_train3 개수:  455 , X_test3 개수:  114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "데이터 셋팅이 마쳤으니 이제 학습을 시켜보자.\n",
        "\n",
        "<br>\n",
        "\n",
        "###**[학습01.] Decision Tree**"
      ],
      "metadata": {
        "id": "y1oCqe04d4IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "# 학습\n",
        "decision_tree.fit(X_train3, y_train3)\n",
        "\n",
        "# 예측\n",
        "y_pred_21 = decision_tree.predict(X_test3)\n",
        "\n",
        "# 정확도\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test3, y_pred_21)\n",
        "print(f'► Decision Tree의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "id": "9mP3n99_d4Ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30698b40-827e-4ede-c8de-5aa1932212b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► Decision Tree의 정확도는 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습02.] Random Forest**"
      ],
      "metadata": {
        "id": "7ey9urj7d4Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "# 학습\n",
        "random_forest.fit(X_train3, y_train3)\n",
        "\n",
        "# 예측\n",
        "y_pred_22 = random_forest.predict(X_test3)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test3, y_pred_22)\n",
        "print(f'► Random Forest의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "id": "TxiLFqNWd4Id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c2dcda-5d4c-4668-a5cf-02d12e57fa7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► Random Forest의 정확도는 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습03.] Support Vector Machine(SVM)**"
      ],
      "metadata": {
        "id": "HrgiAeY0d4Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "# 학습\n",
        "svm_model.fit(X_train3, y_train3)\n",
        "\n",
        "# 예측\n",
        "y_pred_23 = svm_model.predict(X_test3)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test3, y_pred_23)\n",
        "print(f'► SVM의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "id": "Zjfec55Yd4Id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ec1790-e06b-4c25-924f-5cc980d01fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► SVM의 정확도는 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습04.] SGD Classifier**"
      ],
      "metadata": {
        "id": "ECphszGkd4Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "# 학습 \n",
        "sgd_model.fit(X_train3, y_train3)\n",
        "\n",
        "# 예측\n",
        "y_pred_24 = sgd_model.predict(X_test3)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test3, y_pred_24)\n",
        "print(f'► SGD Classifier의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "id": "4Wgm8BZJd4Ie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0febc3-f284-4f8f-9a71-b0d57cf8b54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► SGD Classifier의 정확도는 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "###**[학습05.] Logistic Regression**"
      ],
      "metadata": {
        "id": "0CUFaTDxd4Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 학습 셋팅 '''\n",
        "# 학습\n",
        "logistic_model.fit(X_train3, y_train3)\n",
        "\n",
        "# 예측\n",
        "y_pred_25 = logistic_model.predict(X_test3)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test3, y_pred_25)\n",
        "print(f'► Logistic Regression의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "id": "bRXvbJE3d4Ie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148f7511-70c3-4569-aa08-a1e640ef0116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► Logistic Regression의 정확도는 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "            \n",
        "<br>\n",
        "\n",
        "###**모델 평가**"
      ],
      "metadata": {
        "id": "g6IYX3KEeVzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정확도(accuracy)를 비교할 경우,\n",
        "**Random Forest**의 정확도가 **96.5%**, **SGD Classifier, Logistic Regression**의 정확도가 **95.6%**로 높게 나타났다.\n",
        "\n",
        "오차행렬(confusion_matrix)을 이용한 classification report은 아래 보는 것과 같다. 다섯 모델 모두 비교적 균일한 F1 score와 accuracy와의 차이도 크지 않음을 알 수 있다. \n",
        "\n",
        "다만, 유방암 진단이라는 목표는 실제 악성 환자임에도 이를 파악하지 못하여 적정 치료시기를 놓치는 경우가 가장 위험한 오류라고 볼 수 있다. 따라서 FN 낮아야 하므로 Recall(재현률)을 더 중요시 하는 것이 적정하다고 판단된다. 정확도가 높은 모델 가운데 **SGD Classifier**가 악성(malignant)을 의미하는 '0'의 Recall이 1.0로 재현률에 있어 매우 높은 성과를 보임을 알 수 있다.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VLIbX_UEeVzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Decision Tree report\n",
        "confusion_matrix(y_test3, y_pred_21)\n",
        "print(classification_report(y_test3, y_pred_21))"
      ],
      "metadata": {
        "id": "pa7OS6UzeVzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f70a24-6020-4f0c-8b13-fac70f413cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93        43\n",
            "           1       0.96      0.96      0.96        71\n",
            "\n",
            "    accuracy                           0.95       114\n",
            "   macro avg       0.94      0.94      0.94       114\n",
            "weighted avg       0.95      0.95      0.95       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest report\n",
        "confusion_matrix(y_test3, y_pred_22)\n",
        "print(classification_report(y_test3, y_pred_22))"
      ],
      "metadata": {
        "id": "2VrPe3r9eVze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4747b3-65ad-4a5c-f204-cba8352c74f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95        43\n",
            "           1       0.96      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM report\n",
        "confusion_matrix(y_test3, y_pred_23)\n",
        "print(classification_report(y_test3, y_pred_23))"
      ],
      "metadata": {
        "id": "hJueJD2HeVze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861bf559-1505-4858-8581-d9063e749d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.86      0.92        43\n",
            "           1       0.92      1.00      0.96        71\n",
            "\n",
            "    accuracy                           0.95       114\n",
            "   macro avg       0.96      0.93      0.94       114\n",
            "weighted avg       0.95      0.95      0.95       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Classifier report\n",
        "confusion_matrix(y_test3, y_pred_24)\n",
        "print(classification_report(y_test3, y_pred_24, zero_division=1))"
      ],
      "metadata": {
        "id": "GlcuruQoeVze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4661abc-f5d5-4d44-813c-65ab918f83e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95        43\n",
            "           1       1.00      0.93      0.96        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.95      0.96      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression report\n",
        "confusion_matrix(y_test3, y_pred_25)\n",
        "print(classification_report(y_test3, y_pred_25))"
      ],
      "metadata": {
        "id": "ZX870uHzeVzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9bd8d0-b5e6-469f-dc87-4d745d81c433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94        43\n",
            "           1       0.95      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "SGD Classifier의 훈련 세트와 테스트 세트를 그래프로 그려보자. 에포크를 300까지 늘려갈 때 점수변화를 알아볼 수 있도록 셋팅했다.\n",
        "\n",
        "아래 그래프에서 볼 수 있듯이, 이 학습의 경우 에포크 값을 늘리는 것 자체가 큰 의미는 없다. 100번대에서 양호한 점수에 이르는 것으로 보고 이를 기준으로 다시 한 번 학습을 하겠다. \n",
        "\n",
        "이전 정확도에 비해 소폭 상승하여 "
      ],
      "metadata": {
        "id": "7dNYmHYVQovD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Classifier Score Graph: 300번의 에포크 기록 \n",
        "import numpy as np\n",
        "sc = SGDClassifier(random_state=42)\n",
        "train_score = []\n",
        "test_score = []\n",
        "classes = np.unique(y_train3)\n",
        "\n",
        "for _ in range(0, 300):\n",
        "    sc.partial_fit(X_train3, y_train3, classes=classes)\n",
        "    train_score.append(sc.score(X_train3, y_train3))\n",
        "    test_score.append(sc.score(X_test3, y_test3))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_score, label=\"train score\")\n",
        "plt.plot(test_score, label=\"test score\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U5b8yj33qnWs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "bc294220-ab16-4b9d-c9ff-58aebe60838c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEJCAYAAACZjSCSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hVVfaw333Te70QIAmEXkJooYkKiAWxIPbe64xlHHXU0VHHGb9xfjq2GcuoI5axIzqoKAiKohQJvZMACUloSUjvyd3fH/ueW1IglAvoXe/z3Oeeus86ba291tp7H6W1RhAEQfBfbMdaAEEQBOHYIoZAEATBzxFDIAiC4OeIIRAEQfBzxBAIgiD4OWIIBEEQ/ByfGQKl1BtKqb1KqXXtrFdKqReUUjlKqTVKqeG+kkUQBEFoH196BG8Ck/ez/kygj/N3M/CyD2URBEEQ2iHQVwVrrX9QSvXYzyZTgbe16dG2RCkVq5TqorXetb9yExMTdY8e+ytWEARBaMny5cuLtdb2ttb5zBB0gG5Avsd8gXPZfg1Bjx49yMrK8qVcgiAIvzqUUnntrftFJIuVUjcrpbKUUllFRUXHWhxBEIRfFcfSEBQCKR7zyc5lrdBav6q1ztRaZ9rtbXo2giAIwiFyLA3BLOBqZ+uhMUD5gfIDgiAIwpHHZzkCpdT7wAQgUSlVADwKBAForV8BZgNTgBygBrjOV7IIgiAI7ePLVkOXHWC9Bn7rq+MLgiAIHeMXkSwWBEEQfIcYAkEQBD9HDMGvnfoqWPUeNFTDirfB0dzxfVd/AHXlh37snPnw7ROtf9sXHnqZLanZB98/BUteBofjyJXbEXJ/hD0b9r/N7nWQtwj2boSt33qvK9kKGz9vvU91Maz52JybdQ9WvgvW1wQba2H5W9BUD8vfhIYa89/U0LosrWHFO6YsT9bOgHKPRnqbv4J928109jwo2rL/8xJ+VRzLDmXC0WDdJ/D5nVCcDT8+A+EJ0P+sA+9XshU+vQWmPA2jbjq0Y8+6EyoKAOWxUMPKd+Du9WALOLRyPdk4C777q5lO7AO9Tz38MjvK/26HzoPg0nfb32b+n6E012y3ZS7clw3BEWbd3Idhy9fw+40QleTe5z+nwb5tMOh8WD/TvTx5JNj7whd3w+r3YedKWD4dPr/LrLcFwbArvI+ftwhm3Q4l2XDa42ZZcTZ8cgMMuRymvQyVe+D9SyFpMFz7Jbx7AUR0MrIKfoF4BL92ynaY/7yfzP/ajw9uP+v/YKktNUbg1MfgsTL378I3oHKXqU0fCSr3mP+gCFOLPppU7YWqPfvfpmyHkbFyDzRWm5o3mBp69jegHbDOQ9k3NxojAFDdovOkNb/6ffNfX+m9vqGq9fHXfuT8/8TtMa1xLtv4ufEuLGPT3AQbZpnp2tL9n5fwq0I8gmPNupkQGt26JrttAdSWwaDzjNIs2wFDL/feZs8Gs270zd7LS3NNSOHk+6C8wCwrXGH+N38FdRWmNv7js3Di7yE4vLVc1n7l+bDwHzD4IohNbb3dklcgZRTsXAFJGWYaTCgEoHO69/Z9z4TgSKOglDIKcdB5sGMJlOZBfBpkTQe0qaGO/a0JU2xbAD0nwE/Pg26G6K5wyp+gei+ExsLAc02446s4OOn3sOBvRskFRxhjFBLVWnaHA378Bwy7CiI7w6J/wsCpENcdlv4bek40NfDFL5oQz8gbIXmEUaQRdqPYLeW88XMICgdHkwnHBASakE15gVHQ1vVc8xFEd4PvnwRHoyln8YumrJPvg63fueUrazEiQHURFOd43Oftre9ZYy3Me8wd0tv0pTlGRQHsWAzbvzdeRITdlLfla3flICzWPR3ZyYQS8xZB+oVQVwYh0dBcDwHBoGzQWGMMcF0ZDL7QPIvlBZDQG/asgxHXmutWsMx4M0teMvd30PnGkxv3O/Nc//CUuW7hCTD6VjPf3GDu2amPuT0oMOf3/f/B2NshIsHjXjbDD09DxsWwbgYMnAaJveGnF6DXKZDk8RxmvQGdBkLqGPeyNR9DWBz0OUiPsnA5FGTB6FsObr/qElj8Lxj/BwgKM8saqs05jL4Ffn4VxvwGIhIPrtxDRAzBsaSx1oRP7H1bG4Lv/p+pbQ46D756wCiFIZcZ5Wmx9GXzsg65BEJj3MsXPgMr3jIPerlzOCdHo1HADVVGOQSGmBcuabBRfi2x9tv2Paz/1MSjJ/7Re5t92+Dr+6HzYPPip4yGG+aYdXvWm/9OA733CQ6HtJPNy1OWb4zWoPPMC1C4HHpPgvWfme3WfASjbjHx7yUvGiWx6r9GYdSUGEVTXWSUVuYNsHWBuSZo87Jbyq73qdDvzNbnWJIN3/7VKPBB0+CbPxnlNva38NUfjLI59c8w90/G+ChlDMFXf4AYZ6f4KqchmPMQRHWBplpzrYLCjGGzaunlO4zy3DrfXNt926H/2ea4c/5o5Oh/DuQvdctneWP2/lC0yZxLwc/e1x+Mkt250pS7YRYsfQWik8FmM9fgzP8z4Z6NznURneDcF+CjayB/mdkXjIdT6ezTWbkL5jwM9eXGSFcXG0NR7jk8GBCTCrX7oN8UeNMZcrSes75nwrLXzDOaeQOscobQsr8x55k61vyvfAeiukLlTshdCLvWQEyyOVZSBgy/yn289Z+aEGdgKEy43708dyEs+H+w+UvYtdqEv8b9ztzTHYvhMqcXVbkbvrwHug6Hm+abZQ3VJrwWaYc7V3m/Ywfim0fNsftONhWIjrLsdXMeiX3cFbw1H5plW+ebcwgIhgkPdLzMw0BCQ8eSLXOgodLUnj0TnVqbZeWFRqHuWQv1Fa1fQitRadW+wSihDf8z02s+9t6n10RTq1/7Eezd4F1GS6wabK0zyWgpdk/WfuJctxbQkO+s1Vvbh8aYmntLYrub8svzoWKnqc3t3WCOVbTZGJRTHzPKt2q3+xx2LDaK5/xX3TJWFRnF1nUoXPGRezuA677yPpdW55jvXu/pAXlOV+4ycljz9VUmbGJdv8Zqo0DL8tzlFGfD3k3ua2cx4BxT8927AcbfZ3ILgy+EWxYaI7H2I7N/kEcNuNckuG0RoMxxPM+lttRc4xvnQfdxZt3aj42R+t1a87tzhanlhsa6r8vUfxnDGNkJijeb8BTKnENjDdgHmGX15aaiULrdPKctnz8wBq6hyniNFo4m879+ppFJO7wNmOVFrZ1h1odEG1nDE40C7DnBzMf3dIe2LCyPZe3H7uQ5uMOCu1ab/41fmMoQGMNjJcvXzTTyFGa5Denmr8x9LM013ktHqdjpDnGum9Hx/bT2Pg+LtTO8z2HNR97n6EPEIzgcmhrgs1vdcWpPlIKT7jHKd95jpvZTuMIkDQeea7axHoLGGvj6AaM8B5xtkrn1FWbdkpfcZe7Z4A7POBxuA5D1hnlohlwKXz9oXPW4HqYG2Fjj3j8mFRL7mpBQnbP8vevNS/LZb4zbf/azZt+WL/3eDeaYs+8xnknySPOSxvUwL1Bsd6NI1n1iPJ0Nn0GnQW3XrmKSjfIorTEvZfEWqHC2YNmzDgZfbGQFbyW9ey2kjPFeV73XHX6KSXZvFxID8b1MraotBWbtD07l35ZR8JgOjTHTlpyWsgPY/oP5r9zpVKoYz6AlPSeaMFfRRhNqs4jqbJSfpcST0o135GgyytoWYLyg6iJjlCI6mXtVV27CGQCxKbDhc3Ndx91pvAGva55irovndYpIdFcEOg8y1x6g+1gjI8CEP8JHV5vj1Fe2Pq/AUGNkPA1Bs7P10tqPTW0bzLFDY82zWZprlq2fCd1GGNkCAiH9fBMSybjYPDeDL4bv/w7Tp+BqcLBjEcSlGW/ujTNMghzM9bKeRet/6b/d02+dY45ftMn9rL53ibmW+7ZCZJKRbeZNxpuy6HemOfdV77W+nzXFgDbHWPRPyPm29TZt4Wgy8selmZDn9LNMOXk/mWWl202Z+7Z6nyMYb7X/lI4d5yAQQ3A45Mwziq/rcBNe8GT3WvNyxKYaxRs70zx8iX1NzbCuDLLnGsWWvwR+/jegYNcq7xYk6/9namhFG82L2s/5rZ+yXFOLAeNSgmmeWF1klMzgi+E9p7JRNqOgYpJNvHThP0yNCIwiWPsxbPnKHD/rDdO6pLzAvR+YUEbOPLO+YqcJExVvMYZj33YT7vnhKRN/Li8wSnjkDW1fN0sRWWVnz3Wv0w6j1KxtPJWxa10357p8d2gITEzZUjaxKUYZRnfbj0fQhsK3PJWWy1NPMNe3reT5tu+8z6c9IjvBKQ+be9wy39LnDFN+Tam5x6V5xhuyYsSRncy5NtaYa1Nb6m0IYlJMDR68jYxFTLLTc8Md1oro5A4LeRqC1LHmPoMxCpP+ZLatKzPepuVZpI41z3JYPKz8r1HeuQvd12HnSggIcV+b1LHmObPW15RA3mLofoKZH32ryYsNcFaUhl9tntPGOvd59JwIZzwB8//i3bQ51elFLvsPZF5vDEp5AUx40Dzfxc4WUJ0GmNh7wc8mLAbmWR12pQnFZn/jLrM8H757woS6bIHGQ/EkPBHG/BZ6nwI/Ptfx2rstEPqdZa7rnIeMF4+C3qfB6X8179HJ98H8xw+v+fZBIIbgcFj7kamp3TAXAoK81y34u0lY/vScmbcSf8VbYPca4/41N5iH4c2zAW08iIVPe9euGirNQ95Y7Q5HgLsmFxxltgFTkxh9G5z5pGkBEtHJ1JiTBpvjxSRDp/7OmP5as+++baZNeufBJoyz9hOY9KgJS1n7Wcf41tn8MGeeeQlsQTDwPAiPdx4/F2bfa6Yvfc/kPtrCUkQWW+a2WJ/sVvYlOeYcPNcFRxjlU7LNvCgRnbzLritzG5KY5PYNQVm++7/MQ/lbyr5qj6mVgbkHW75yK05Ptn3fRuHK5AmaG9zeQ4TdJNMHnN16886DzH9DpZE50u40BJ3c+1YXGe+t0wCzrHS7hyFwnm+nQe6yPLHWh8SYxglgjmHhmctJGW3+o7uZ8sfd5S3nW+eY6QkPGE8GYOhlxng9n2Hme00ysW5PD6LbCPPsOBpNhWTrt+7zBUjoBRe85iFzN7jyk9bnAnBZGzV0MGEvgGmvuJf1GNd6u/Zq1Sf+zj2dtwimn2mM70VvmVxWexxqs+WrZrZeduF/zH975+gD/DdHUJBlFHBdhUlO/TMTXj6x7TBP1V54Y7Kpvbxxptm+vtLEFged39oIgIn9ok2irHO6qV3bBxjl+fZ58PUfTU2k+zjjBkYmmRcrMsl4E2Hx7rI6p5sXfPdaU/N6fgjMugNQ7j4BSc4X0KoNBgRC+gVmOnWs+bdeuAznNgPOMTLuXW/kzbjYtC55brAJPVj7DXC++LvXmuM4mkzStvepbiMAJvGpAqDLkPaNgKccFnk/GgWlbO71Vu1+x5K2941JditlT4XmuR6MYTiQR1BTbFx1gKY62LnKvU3+UqMM7f2dsi5qo5x87/sVFm+UWudBpuYfHGmWR+xnCHVP5R2T7DYAkR6GwMoRxKS4DUBLQzD4wrbLb3ldWspjHT8wzMgcFt860d9y/5YGPborrhBO3za+Uhub6s4Z9ZpknpWWZR5PpIwxYaLgKOh7xrGWxqf4r0ew5CXjxm6cZRJEZXmm9rb2IzjhDu9tV39g3OGPrjK1sp9fhR4nGqXRlhsORhGc+phJfmZeb5Rop4GmVp/vTJwNvtC406c9boxJQBCc9bRJdPWeZFo31FeY/ZobTI103mMmDNXndOg80LiTSelmfuPn0G24W4Zxd5rY9qibjMLoMsQsH36NccHH/MbUWnWzccODwt3ueWAwnHCnqfkPvcy8wJW7zLINn5naX8smcxGJJlSU0Gv/1z7CbkIGzfVGGehmSDvJKOCKAreCiUlxGwJbkKlJeq7b/KW7PItWhiDZyN3c2Npgl+e7y92x1GN6ifd0Qi93eZ7rYlJNshSMEV36ijFmU1804Y/gcFPRmPeoid3vzxCEx5tWR5W7TLnWtp6hIau5aGyKu/+CZQhSxxqPMrOdQXxjrevmaQg6ua9tQm/3eqVg8t9MLL0l0d3angZzfaO6mFxJ2skmf9BU53G9ks19K8szx0vobZLVLQ3K8YLNBmc/Yyp9VhPPXyn+aQjqq2DTbDO96j2TpB14nglDrGnDEFhJXau1w4ZZzgRpqrvdfFuceLd72tqu+9jWsXMreQym9m3VwH981hiOzgNNfHLOQ0aGc543TSctOjtrbvZ+3uVGd4WJD5ppz2ZoYbFw6qNm+uxnvPc58+/e8+PvM/+T/uRe1mk/TdpGXNP+Ogubzbj9VicrMAa1utgYAkvBxKa449rdhpvauaXIYj2Uh1doyMMTsLbTDqNgPePyjmaT67DKbaqF5FEmdtxUa8Ij+UudMflUd7lNtdAt0yQnY5LdhmD4NebZCQprHXZY/C9Tmw+J3P916TTQaQiS3V6OKzTk0Z48Jtn0/Aa3IQgMgUmPtF92TBuGwNPbsKat9UMubbucwBDjtWoHBIW2cZxkYwjiupvncddq9zWOTTG/PMx/54HGEMQep4YAfvWegIV/hoZyvjEvdNrJJlNfV+ZMsF5k4vfrP4VnBpqa+d9SzLK08WbftJNNUi53odn+YNocHywxKe44bXQXU2u2BbmTab9kYpxKwQoH9T3DzIfFuxWmpbyUzTtu7bkO3EoMPGq+Lf6fy4A/x7l/jyeYWqqVqARjpC1SRrtli00xYapgZ6e0hF7GyEZ1dm/feaAzkd2GUovsbH4HwgrPxHQzytbaF9zz1jm5QkMeIan94WkYLSzjEpFo8i7BUR1TypZCb29dhN0YxM7pJtTUdbjx/KK6eBsk1/kex4bAT/BPj6CmxPyf+ZQJDQVHmFBMlwyY+5AZQ6ahyoy101gLEx82YZB1n5i4+6p3jcs/8kbfyjnpEe+u/lOeNsnd8A6+/Mczp//F9LwNiTIx+qAwGH+/aZpqMeZW470k9DYGuNsId6Iz4xLTNDE83rum3+8sOOcF07wVTA7m1D+3PfxCQLC5hzHJpqdn5nWQ0MfE4YdfZZLlJTnujnzTXjY13PQLTCuTCLvZ3+q1POUfrZttAkx8yP3M7Y+xv3WeY4zpZBSV5DY2/c+CyodN+C4pw52rsAzCgYjuAhf8x7S6sWiZh7ho+oHDegCT/w6000Jm/APm2oBp+TJwqklup51sQkcjbzAGIDTGdDKL7e5uGCAcM5Q+Sh0WjhSZmZk6Kyvr8Ar5+TXTuuXeHO9EI8DbU03bXos+p8MVR3kMG0E4EKveN31YLvvQ3aT4YKkuhqd6uQefE37VKKWWa60z21rnn6Ehqx2zauP0My4x/+nO1heDLz46MgnCwWB5Cp5hsYMlLM6Ebtrq/S0cdbTWLM/bx7GonPtnaMi60G3F9zMuMS5/r1NMwqzXpKMrmyB0hLQJcNVn0HXYoZdhC4DrvjS9WYVjTlZeKRe9spgXLx/Olj2VXD8ujZjwNpqm+wDxCFpiC4A+p3n8++clEtxU1DXS2Hx4H71paHJQVd904A07is1mhi853MYK3Ua0m3OqrGukrrHtDxn969tsHv3fOl75fiuPzfIeh2rG8gJufWd5u4dclFPMhS8vor7JlL1iRykXvLyIyrpGALbsqWTqiz9RVFl/KGf0i2VFnskH/t+cTTw/P5sv1u48asf2Ty23P0MgCC3IeGwuN751eHmpG9/OIv3ROcfE7T8UtNac9+JP3P3hqjbXPz13C28tzuO/S/J4d2ke1R5G7t6PV/P1+t3sLGtjvCXgb19tIiuvlGXbjeL7OKuA5Xml/JhdDMCnKwtZnV/GNxsO8K2HDjB/4x7ySqq9li3KKW61bMWOUjbvbvF9B2Dz7kpW55cdthwdYW2hGU4ir8SMD7a24OgMLwF+bwiOXNNPzxf8cF72d5bk8c6SvANvCCzPK+Ufcze3Wp69p5I/frrWVePqCA6HxuE4PCX17DdbWLB574E39AHtXfPDVbxWjfj7LUU0HaRXoLV2Hf+HLaYPyvbiai+Zmh2aj7LyuX/GGvZW1rUqo9mhaW5xbzzvled0ZV0j989Yw5JtJa7jtydXs0d5T361ie+d8r0wP5u3F+eSlVfK1qJqvlq3m427Kry296SgtJbGZs2irSWu4yVFh3qdc0v6dDbNgxfmFKG1dm1nyfD9Zmt+L442zn9/99Rz3dz1u7nhrSwmP7fQZWR2l9dxzfSfufvDVa5tG5sd3PRWFrf+d3mr87zrg5Xc+HZWu+fvydJtJTw2a/0hv0eWIbBYcxQNgZ/mCNweQUOTgx37aujd6QCdfTxYV1hOaU0Do9Liyd5TRXhwANe/uYzrT0wjOS6MP85cx6i0eP4yNZ2Y8CAKy2pJiAimqLKe6NAgV9xv8+5K7v14NUNTYvnNxF4s2FzEnz4zA3/tKKkme28V068dyR8/Xce2oio+uHkMysN4zVxRwLtLd3DHKX0IDnTb9P/8uJ0PluXTr3MU15zQg4YmB5t2VzCgSzT//DaHGVn5fHzbCZRWNzCoazRv/JTL03M20+RwcPPJPfn9af0IsHXMSGbvqaRTdCi5xdU8Pz+b2PAgVj1yeoevZXs8+dUmFmYX8eWdJx1w2y/W7OSpOZt598bRJMe5B//7btNe7vpgJf+7/UTSEiNa7VdUWc/IJ+Yx/bqRTOznTro2NDkY8ue5PHrOQE7s4+7I9dPWEsb3tXPVf5Yyonscl4xMISwogMiQQK6dvozuCeHUNznYV91AREggFbWNBNoUwYE2okMDqahr4sa3s4gKCeTFK4bz1y828vX63W55N+/l41vH8vKCrazcUcY9p/fltndX0OzQxIQF8c3dJ7O6oJw731+JTcHLV47gprez0MBdk/oQFRrIh1n5fLQ8n3tP78d7S3dw+ehU3lmcx+mDOvPQWQNYsLmIhz5dS0l1A+cN7cZj5wzile+38t7SPF67OpNnvjHfKo4KDSQk0EaATXHm8+Yb05nd4/jHxUPontD6Wt70dha97BE8d8kwEqOC2V1Rx/9W7aRzdCgp8WF0jQ3j5+37SIoJpaHJvH/fbNhDb3skhWW1hAbZ+G7zXr5cs4sNuyoIDbIxZ/0eej80G4eG4EAbt43vRVhwAC8v2MrEfnZ+zCnm8anpTBncBYDPV+/kz5+v59QBnbnn9H48OHMtA7pEU1HbyIvf5XBin0ReX7iNxmbNih1lLN2+jzE9E/gxp5iS6gZKqht4eUEOF4xIpktMGJt2V7DJ6SXMWl3I8NQ4r3P/22xz/548P4MxPeN57PMNbNxVwUl9Epk0wLvfSH1TMz9v30dTs9tIRIQEMrJHHEopymsaySupYXRaPEu37yMpOpQteyqpa2wmNOgIfNL1APilIaisayQKWF1YyUNfrGD9zgq+u2cCPZzKor6pmbcX5XHV2O68u3QHk9OTCAsK4N0leYQGBfDEbDNEb2JkCMVV9QTYFM0OzZ8/30CzQ9M9IZzZa3eRlbuPv1+YwW/+u4LuieGsK6zg1AGdef0a04Lrrg9Wsml3JZv3VPJDdpHLJQT436qd7K2s50//W8f7P5veq9ZDubawnIszU1yud0l1PV1i3F3grQfnzUW5nDk4ievfXMa6wgqXvGBq7zOWF9A/KYpNuyuZ0M9OREggL363lYzkWE4b0Jk3F+UydWhXZq/dxRmDkugUHcpHWfmM651It9gwXpifzbPzttApKoTESDPKZPf4cL5Ys5Os3PY/dTihn50J/bxbu6wrLKewrJYzBiVRUdfIK9+bnrN1jc3srajn59x9jE6L581FucRHBHPduB6EBweys6yWBz9ZS2V9E3+bvYkR3eO4YEQyr3y/lZcXmDLWFJS1aQjmbzShh/eX7mBiv068t3QHkwZ0YntxNbWNzTzzzRYGdIl2bf/dpr2kd41mYXYx24ureW5eNv2Tojg7ows/5hSzKj+QJoeDusbWnkOXmFAq6prYVmRCEhOeWoBNKa4fl8bArtH06xzFha8sYtI/vqfJWaP8at1uHFrzmwm9eGnBVl7/cTsfZeUTFhzAvuoGPl1ZSH2Tg9Fp8Tw1ZzOhQTZ62SOIDAnkqTnGU7SWv704j7DgAF5fuJ0BXaKYMrgLby/OIyLEPCsVdU1cM90MffL70/ry0oIcpgzuwrRh3ViVX0Z9UzPvLM5jyvMLefaSoShl2lyEBQUwoZ+dr9btpqKuibs/WkVtg/GiFm8rYfG2EmLCgrjhxDSe+WYLIYE2hqTEArCtqJr7ZqwhwKa445Q+PDVnM799z3xJ757T+vHE7I1M7NeJISmxrN9ZzvPzzXhQvewRfLZqJ+HBAdz/yRp+3r6PgtJa5m3cQ8/ECD7MyufbTXspqW7gtWsy+WFLEc/Pz+annGLeWZLHmelJLMvdxxNfbmTGbWP58Od8okMDsUeF8PTcLby8YCvThndjy54qAmyK0EAbd3+4mqAAxfnDkgkLDqC2oZkPs/IJDw7g8teXMLFfJzbuqiDQpnhi9kYWOj0QiyXbSlzvrycn9Eqgb+coNuw0w8LffVpfduyrISTQxl0frOLR/63nlvE9sUeF8P9mb+Tmk3u1+SwfLj41BEqpycDzQADwutb6yRbruwNvAHZgH3Cl1rqdEcKOHDl7KhgGXPTvJdiCQtAaNu2uQCl46NN1XJSZzBOzN1JR18g/v83hL19sICo0kMo6EwcdkhLLjSem8fz8bKYMTuLn7fu4bFQqM1cUMCotnnvP6MemXZX85t0VXP3Gz2gN6wrNjf4pxzwgVfVNbN5Tyfi+dr7fYozAI2cPJCEymLs+WMVeZ6Lsv0t2MDw1ljUF5Xy2qpAlW0tYXVBOdGgQO8tMKGHWqp2sLSznhUuHYbMpl7LfXlzN459vYF1hBdee0IMfthTx8FkDuG/Gar5YYxJRm3YbGaZfO5LK+ia+XLOLvJJq1haW8/gXG1iZX8bnq3dSUdfElWO684cZa/jNhF7cfVpfnpu3hRN7J1Ja00D+PmOU6hodPP75BspqGgkNah15bGzWvLs0j//99kT6do501fb+9W0OP+UUM+FPdp74wv2hnd3ldfx3SR6v/7idG09M4z8/mvF2Pl+9k8/vOJG563dTWd/Eib0T+XLtLr5cu4sd+2p4c1Gu6561l3RcunYSDCEAACAASURBVN18rCQhMoQ9FXX88dO1/L6qL7XOcFBaYoTrWgIUlNaQ5UzoFZTWuq5fQ5ODqJBAKp1x8hHd4+iREEFMWBBv/GTkLa6qJ9CmGNc7kWGpsXy/pYgnzhvMwK5uQ/OXqelMX5TL5EFJPDtvCyt3lJIQEcIfJvdnYXYxr/6wjbCgAF67OpMr/7OUlTuMLP+5diT3z1jDl2t3cf7wZM5MT+LOD1ZyxejuvLd0B787tQ//9/Vm3vhxO80Ozd8vyGBQ1xi+2bDHZbBPH9iZuRv2kNk9jjsn9eHqsd0JDQogNCiAk/uavjZXjO7OFa8v5cmvNqG18RqmDevGA2f258kLMnhpQQ7Tf8wlMEBxxehULs5M4bNVhUz/KZdNu83zX9/kINv53N93Rj+aHJq48CBS4sKZ0M9OY7MmKjSQXvZIzh3alc7OMJPWmuy9VTQ7NP2ToigorcWhNTe8lcXMFQUEBdi4a1If7jilN698v5Wn525hbM8EhqfGERcezHPzsrl2+s9EhQbx53MHsWJHKbf+dwVj//Yt+6obuG1CL246qSc5e6t48bscZq0y78fFmcl0igplxY5SosOC+GrdLtf9OqlPIv+8bBhPzdnMF2t2MSQllitGpfL/vtrIzBXeaiwuIpgXLhtGarzbY12eV8or329lXWE5YcEBPHn+YMb0TGBMzwRKqupJiQ9j5soCPltVSFhwABW1jQxLiftlGQKlVADwInAaUAAsU0rN0lp7fhLraeBtrfVbSqlTgL8BV7Uu7QjjDA1NyejGXacPZOLTC9haVM3MFYX8mFPsCotk7zG9UYelxtIzMZKzh3Thx+xirj2hBynx4ZwzxLv99TUn9HBND0mJ5V+XD+PCVxYzoZ+dE3onMn/jHhZvK6GqvokNOyvQGq4e253txdUE2BTXnNCDXeXuBNv149LoHB3CdePSuPmdLL5cs8sVAnrkf+uocda8PliWz/biau6f3J+U+HCKKs1DlL+vltlrd9HTHsFj57pHt3zl+61s2l1JbHgQl49K5foT01BKERUSSHhwALvL61mWa5Tkl06DUVBay+5yY3jyS2vZW1mPQ8OUwV24bJTp2fvgzLXMXruLirpG7jylD3ef1noE0n3VDZzx3A/85YsNpMSHkVtcw0e3jiW/tIbK+ibu+Wg1X6zZxYAu0WzcVcGu8jryS42ntCyvlK4xodx8ck8e+3wDeSXV5JfWEhYUwNMXDeHhz9Yyb+NeV55i6R8nMeIv81xye+JwuGPTeyvqyN9njrGzrNYVm62qb6KkynxkxVI+Wbn7XB4gQEyYCf1NG9aNGcsLCLAp3r1xtMsr62mP4OHP1tHYrPnjlP7cfLLpufu7U1tfm4tHpnDxyBS2FlXx7Lwt5JbUMLib+QTp1KFdWVtYzsNnD2B0z3gz9H9JDVEhgUSGBPLEtHQ6R4dy2ahU4iOC+eIOE1Kz7s3P2/exeU8l9qgQBjq9nM7Roa649K0TejEsNY4R3U1P5djw4FbydY0NY3hqHDNXGiX3+NRBTBvmHruoU1QoDc0OGppNnmBISizbis07ZNV4AUprGkmIDCa9W4xX+YO6es9bRgBAKUXfzu7vTqc4Feq8349vJedtE3o7n00zLEdaYgT3nNaXnKIqrhrTnU7RoUxO78LjUwexLLeUk/okctGIZJRSjEqLZ1TafsYPa4Mnpg3miWmDXfMXj+zYkBlDU2K54cS2m+4mRIaw8A+nsLu8jn99l01NfTNXjEllRHffjCrgS49gFJCjtd4GoJT6AJgKeBqCgcDvndPfAZ/5UB43TkPwyDmDiI8Kp2tMKFv3VrHTqYTLak0ztq1F5iH+91Uj6BRlHsqJ/TregWdYahyzbh9HSnw40aFB9EyMYNHWEtYXlrtewMHJMbx53UgCbIoAm6JbbJirdnnHKb2JizAv5Ml97CzYXIRSEBse5PIYwNT8wdROU+LDKaqqJ6NbLA4HFJbVMrLFw9PPGQ4a39fOHyb3dy1XSpEUE8ruiloKy4xitPJeBaU17K5wGoJ9Nex2XqukGPfLao8Mptx57brEtDEgGRAfEcw5GV157+c8ckuq2VVe56WIv1y7ixN7J/LnqYOY9I/v2VNR5/I21heWMzQl1hVayC2uIX9fDclxYSTFhPLSFSMY8MjX5JbU0Dk6hPDgQLrEhLKrorUh2FZcRUm1UfK7K9zGZvOeSjbsMkprT0UdRU6PICM5hq/X7WZZbikjUuOob2pmdUG563z7JUUxvq/dVYu28FRm8REhbV6TliR57GNd36vGdqdfUhQn9k5EKYU9MoS9lfWu9bHhwTxyThvDRjsZ39fOv3/Yxvi+dleeqUtMKKucLWI6RYVw24QDDy/RJSbU1Q2npbHoHO0+v1jnc2uPNPJZ92RPhbmecW0YmiNFgE1x56Q+XsvuaDEPcPXYHlw9tofP5DgSJMWE8tfzBh94w8PEl62GugGe3wgscC7zZDVwvnN6GhCllEpoWZBS6malVJZSKquoqO2WCAeF0xDYnH0EenWKZGtRFYVOd3/rXmMAcp1NzA7noR3UNYboUJMctmpAawvLWVdYTlJ0KJ2iQulpj3QloZRS9EuKIjkuzGUEAEalGWWuNVw0ou3x2zc73e+iynrsUSGM7GFqd5k9vMej6Z9kaoSDW9TIwLzoO8vqyMotJTrUXU/wVP4FpTXsctayPRV+YpRbESS1YwjAKNW6RoerjM/X7KLCGXbT2pyrpQw9PYImh6ZrbBg9nNcq1+kRWLXD4EAb3WJNrsS6nkkxoW16BDl7zb1N7xbN7nK3sbGaCqZ3i6a4qoFd5bVEhgTS0x5JRV0T6wrLGZYay8zfjOMvU91eVtfYMF69OpMXLvPu4OWpHBMiO/YcRYQEuq69dX1DAgM4qY+3ErfOryNk9ojnzPQkLh/tHpfJc18rx3MgPPeJb2UIWq+zezwT6R41/vgI3xkC4eA51s1H7wXGK6VWAuOBQqBVm0et9ata60ytdabdvp8x3TuIdhoC5Tz9XvZI1hSWU1pjandWxx8rXhkUcGQukz0qhO4J4bzy/Va+Wrfb5Ya35KGzBvB/F2R4LeufFEVEsKlpTh3ajaCA1q16Nu02rQwq65qwR4VwQu9EbArG9PS2rUOdNeq2jp8UHcaGnRWUVDdw7bg0Am0Ke1QIhWW1LkNZXNXAdmfSs0u0O0md4FHj9Uxet8QzJKAUvNuiuWxmjzgiQgKJCg1k8+4KV24GjMKNDQ8iOjSQ3JJqCpwegUX3BGMU0ixDEN22IbC8vXG9EimpbnDNWx7QuN6mtdD6nRUkRAbT1WlgmhyajORYAmzKtQygW2yYy6vzxFM5JhyE8rOuX3uK3lrenufVkuBAGy9fOYLhqe57bu0bHRrY4ZYpnsdrWUHqHOW5zlR+PA1Bv6Qo1/WJPUo9ZoWO4UtDUAh4BsuSnctcaK13aq3P11oPAx5yLvN97w2tcWiFcirTXvaIdj83eqRrLq9cacJMJ/RK4NFz23blh6XGcULvRK9lgQE2hnePIyjAeAxWrT7M4wXevLvSlRi1R4Zw4fBk5t8zwVVjthjbK4Fv7xnPsNQ2DEFMCA3O9vLnDunKvN+P585TetPYrFnl0a45K6+U0CAb0WFuryHRo8brGd5oSc/ECJdRm5LehW3O0Fbn6BACbYphKUauLjGhLGvR+qhbbChKKXokRrCmoJzK+iZSPJqMWt5C98Rw5/mEsqeizqttd7NDs7Woii4xofRyNhv2bOUUaFOMdnpg6wrLSYwMoVus+3wykmNcZVt4GgVPEiKCXd1VDuZZ6mzV+Nu5jtby/V3nAx7Dua+nsu7oPgCxEd7KvJNnaMhpJGLDggh0Kv/O0aHYnZ6HL0NDwsHjS0OwDOijlEpTSgUDlwKzPDdQSiUq5ere+yCmBZHPUVrjQGFzvqGTBnTmzPQkpg7t6gotWBzpB3ZAl2hm33US068b5co7dJRbx/fivjP6ERRgY0hKDKFBNvo6O+ekxIexrbjaFc6yR4Vgs6l2Wxj0tLfdbyLJWRONjwimlz2CHokRrjBLVu4+10u9LHcfXWLCvPo1WKGhsKAALwPREptNkZEcS097hFeo4pGzB3H/5P6EOY1EUkwYhc4mspEhpryuHqEfK6mbEt/aI7AMQpeYUJocmi/W7sLh0CzPK2XEX79h5opCetkjXYq0sKzWdYzuCeEu49LYrEmIcHsEMWFBLg/E2jc0yOaqAbckMMDmCrskdDBHANAlev+hnySXx3DoX86yvI6DMQSWRxBoM40LPAkNCnCFtOKcRsJmU67zt0eFuEJl4hEcX/jMEGitm4DbgTnARuAjrfV6pdTjSinryyoTgM1KqS1AZ+AJX8njLVyz0xCY2a6xYbx85Qiev3QY3eK8X6zjKZY5rneiq9XJXZP68t8bRmN3GpPLR3Wn2aH5rzPMcjAvtyeWAsrsHudS8pZHUdPQzGBnbbimoblVbTTRqei6xIR6GYi2eGJaOi9ePtwrbHVWRhduOrlnK1nAneewFHKPBLcX4NmJbGhKLIE2xSBns0xLUd75/kr+/cM27v5wFWXOEGBPe4RXqMM6Ri97pHf8PCqETlGhBNgUGckxrnOLjwgmOMDkJfZ3vp2jQwgLCnAZuI7gDv20rei7HGRoaH9l2A+iQmKdc2x4UJvnbHkMnhUoy1OwR4XQqY31wrHHp/0ItNazgdktlj3iMT0DmOFLGdqRC+3hEXjSMo57vD6w9qgQ7FEhJC43TfnOzujCu0vzmLPedJLqdIiGwFJAI3u4Wxolx4WZ1jfldfSyR1Je08i24moiWtQIo8MCCQpQHUpgenokj08d5Gr55MnE/naWbC+htz2SvklRLNhc5DIE4/va+XLNLmLDg7x6hWf2iGf1o6e7ZBuWGsvYngnsLK/l719vwqbgnCFd+Xz1TmLDgkhNCCezexwl1Q1cOjKFBZuL6NUpkqjQICb170ROURUn90kkwKa4ODOZ0Wluw2W1smovLGTROSrUZXw6yvh+dtYUlHnlPzwZmRbP6LR4V5jqUOgUHYJSB/esWOfs2ZPdk87RoeSX1njlHKxwkD3S7REcr++Vv+KXPYvRDjSqzaGGWrbsiI84vl3Y5LgwwoMDSIoJ5fxh3Xjh2xwuH516yB7BwC7RPHBmfy7KdLdMCgqw8dSFQ7jyP0tJiQvnitGpTHtpkVfnGDBKIjkuvM0hCPZHe034Jqd3YXK6GT5gR0kNKXHhxISZ+5HZI55v753Q5n6eBioxMoT3bx7DvA17uPHtLH4zoTd3ndqHgV2iuWRkCiGBAcy4zf25yr+cl87EfqZBwn+uHelV7t/O907gAzx6zsADhjl+e0pvig9yJM3hqXFMv6799uzdYsP48Jax7a7vCCGBAfzfBRntNlpojx6JEe2O95OaEN5qsDnrWbRHhdA9PoKQQJuEho4z/PILZStev53++R8R+KfdrWo2z3yzhRfmZ7tqwH+Y3I/fTOh9WMfzJTUNTewur6OnPZKGJgfZeytbdcw5UmzaXUH3+AjCggPI3lNJ19iwVl5B/r4ar/GUjiey91TSyx6JrYPjKAlts7eyDjSuMI8n5TWNVNQ1ejVQ+HDZDt5enMeXd55EXWMzBaW1BzW2l3Bk2N8XyvzSI1Da4ZUj8MQKDfWyR7KrvK5VW+njjfDgQFeYJTjQ5jMjAO7+BwB9PHp5etKyhdLxRHsyCwfH/ho5xIS3rgRcMjKVS0aaRgGhQQFiBI5DjnU/gmODyxC0kSNwhoashzXuOEoWC4Ig+AK/NQS0kyMYkhzL4G4xXDgimYzkmDZ73wqCIPya8MvQkOURtNX8LSU+nM/vOBGAWbefeLQlEwRBOOr4qUcADiRhKAiCAP5qCHCg/fXUBUEQWuCf2tAZGhIEQRD82BCIRyAIgmDwS22ocPDL6kYnCILgO/zSEOBw4PDTUxcEQWiJ32pDLTkCQRAEwF8NgXbgOMAwyYIgCP6CXxoCJc1HBUEQXPinNtSSLBYEQbDwS0OgpPmoIAiCC7/VhpIsFgRBMPinIdDSfFQQBMHCL7Wh0g60tBoSBEEA/NQQgPkegSAIguCnhkBpLaEhQRAEJ/6pDbWWZLEgCIITvzQEpkOZGAJBEATwsSFQSk1WSm1WSuUopR5oY32qUuo7pdRKpdQapdQUX8rjQmu08ksbKAiC0AqfaUOlVADwInAmMBC4TCk1sMVmDwMfaa2HAZcCL/lKHi/ZxCMQBEFw4ctq8SggR2u9TWvdAHwATG2xjQaindMxwE4fyuPC9CwWQyAIggC+NQTdgHyP+QLnMk8eA65UShUAs4E72ipIKXWzUipLKZVVVFR0+JJJaEgQBMHFsdaGlwFvaq2TgSnAO0q11tBa61e11pla60y73X7YB1VIqyFBEAQLXxqCQiDFYz7ZucyTG4CPALTWi4FQINGHMhlk0DlBEAQXvtSGy4A+Sqk0pVQwJhk8q8U2O4BJAEqpARhDcARiP/tHoWWICUEQBCc+MwRa6ybgdmAOsBHTOmi9UupxpdS5zs3uAW5SSq0G3geu1Vr7/FMBkiwWBEFwE+jLwrXWszFJYM9lj3hMbwDG+VKGtlA4gICjfVhBEITjEj8NlEurIUEQBAu/1IZKa0kWC4IgOPFLbaiQ7xEIgiBY+Kch0Br5HoEgCILBPw0B0o9AEATBwk+1ofQjEARBsPBLQyDJYkEQBDd+qQ1NstgvT10QBKEVfqkNldYgoSFBEATAXw0BDqTVkCAIgsFPDYHG4Z+nLgiC0Aq/1IYKCQ0JgiBY+KchkFZDgiAILvxSGyocIK2GBEEQAL81BNKhTBAEwcI/DYGWVkOCIAgWHTIESqmZSqmz2vqw/C8RJd8jEARBcNFRbfgScDmQrZR6UinVz4cy+RzTakgMgSAIAnTQEGit52mtrwCGA7nAPKXUIqXUdUqpIF8K6AsUWr5ZLAiC4KTD1WKlVAJwLXAjsBJ4HmMYvvGJZD7EJq2GBEEQXHTo4/VKqU+BfsA7wDla613OVR8qpbJ8JZyvUMiHaQRBECw6ZAiAF7TW37W1QmudeQTlOSooLcliQRAEi45qw4FKqVhrRikVp5T6jY9k8jmmQ5l4BIIgCNBxQ3CT1rrMmtFalwI3+UYk3yPNRwVBENx0VBsGKOWuQiulAoDgA+2klJqslNqslMpRSj3QxvpnlVKrnL8tSqmytso50tjQ+GlfOkEQhFZ0NEfwNSYx/G/n/C3OZe3iNBYvAqcBBcAypdQsrfUGaxut9d0e298BDDsI2Q8ZCQ0JgiC46aghuB+j/G9zzn8DvH6AfUYBOVrrbQBKqQ+AqcCGdra/DHi0g/IcFjbpUCYIguCiQ4ZAa+0AXnb+Oko3IN9jvgAY3daGSqnuQBrwbTvrbwZuBkhNTT0IEdpGcgSCIAhuOjrWUB+l1Ayl1Aal1DbrdwTluBSYobVubmul1vpVrXWm1jrTbrcf9sGkH4EgCIKbjlaLp2O8gSZgIvA28N8D7FMIpHjMJzuXtcWlwPsdlOWwkdCQIAiCm45qwzCt9XxAaa3ztNaPAWcdYJ9lQB+lVJpSKhij7Ge13Egp1R+IAxZ3XOzDQ5LFgiAIbjqaLK53DkGdrZS6HVOzj9zfDlrrJue2c4AA4A2t9Xql1ONAltbaMgqXAh9orfWhncLBIx6BIAiCm44agruAcOBO4C+Y8NA1B9pJaz0bmN1i2SMt5h/roAxHDDEEgiAIbg5oCJz9AS7RWt8LVAHX+VwqX2I5HmIIBEEQgA7kCJwteU48CrIcHbTD/EuOQBAEAeh4aGilUmoW8DFQbS3UWs/0iVS+xGkIpB+BIAiCoaOGIBQoAU7xWKaBX6whkH4EgiAIho72LP5l5wU8cYWGxCMQBEGAjn+hbDrGA/BCa339EZfI1ziTxUoMgSAIAtDx0NAXHtOhwDRg55EX5yggHoEgCIIXHQ0NfeI5r5R6H/jRJxL5GkkWC4IgeHGo2rAP0OlICnK0cI9rJ8liQRAE6HiOoBLvHMFuzDcKfnE4HJoAJEcgCIJg0dHQUJSvBTlaaId0KBMEQfCko98jmKaUivGYj1VKnec7sXyHwzIEtoBjK4ggCMJxQkfjI49qrcutGa11GUfps5JHGofDmSMQj0AQBAHouCFoa7uONj09rtDSfFQQBMGLjmrDLKXUM0qpXs7fM8ByXwrmK7TTI1DiEQiCIAAdNwR3AA3Ah8AHQB3wW18J5UscDhmGWhAEwZOOthqqBh7wsSxHBQkNCYIgeNPRVkPfKKViPebjlFJzfCeW73A3HxVDIAiCAB0PDSU6WwoBoLUu5Zfas9iVIxBDIAiCAB03BA6lVKo1o5TqQRujkf4SsJqPKpskiwVBEKDjTUAfAn5USn2PGaTnJOBmn0nlQ7SVLD7kYZYEQRB+XXQ0Wfy1UioTo/xXAp8Btb4UzFdIslgQBMGbjg46dyNwF5AMrALGAIvx/nTlLwKXIZAhJgRBEICOx0fuAkYCeVrricAwoGz/u4BSarJSarNSKkcp1WbzU6XUxUqpDUqp9Uqp9zos+SEiHcoEQRC86WiOoE5rXaeUQikVorXepJTqt78dlFIBwIvAaUABsEwpNUtrvcFjmz7Ag8A4rXWpUsrnLZEc8qlKQRAELzpqCAqc/Qg+A75RSpUCeQfYZxSQo7XeBqCU+gCYCmzw2OYm4EVnc1S01nsPRvhDQTfLoHOCIAiedDRZPM05+ZhS6jsgBvj6ALt1A/I95guA0S226QuglPoJCAAe01ofqNzDQmtriAnJEQiCIMAhjCCqtf7+CB+/DzABk4j+QSk12LPzGoBS6maczVVTU1NblnFQaOlHIAiC4IUvA+WFQIrHfLJzmScFwCytdaPWejuwBWMYvNBav6q1ztRaZ9rt9sMSSjwCQRAEb3xpCJYBfZRSaUqpYOBSYFaLbT7DeAMopRIxoaJtPpTJ/fF6yREIgiAAPjQEWusm4HZgDrAR+EhrvV4p9bhS6lznZnOAEqXUBuA74D6tdYmvZAJ3aMgmrYYEQRAAH39lTGs9G5jdYtkjHtMa+L3zd1TQrhEmxBAIgiCAHw64I6OPCoIgeON32lC+RyAIguCN32lDa6whJaEhQRAEwA8NAZYhEI9AEAQB8ENDIDkCQRAEb/xOG1rfpZHQkCAIgsHvtKG7Q5nfnbogCEKb+J82tFoNiUcgCIIA+KMhkGSxIAiCF36nDa1+BDLEhCAIgsHvtKH1hTJkGGpBEATADw2BlSxWMgy1IAgC4IeGwEoWS/NRQRAEg99pQxliQhAEwRu/04YuQyAfphEEQQD80BBYHySQHIEgCILB/wyB5AgEQRC88DttKDkCQRAEb/xOG1qGwCahIUEQBMAPDYFrrCHJFQuCIAD+aAikQ5kgCIIXfmcItNVqKMDvTl0QBKFN/E8bOg2BzSYegSAIAvihIXB/qlKSBIIgCOBjQ6CUmqyU2qyUylFKPdDG+muVUkVKqVXO342+lAc8m4+KRyAIggAQ6KuClcnGvgicBhQAy5RSs7TWG1ps+qHW+nZfydEKK0cgzYYEQRAA33oEo4AcrfU2rXUD8AEw1YfH6xjiEQiCIHjhS0PQDcj3mC9wLmvJBUqpNUqpGUqpFB/KY7A6lEnPYkEQBODYJ4s/B3porTOAb4C32tpIKXWzUipLKZVVVFR0WAe0NdWZiaCwwypHEATh14IvDUEh4FnDT3Yuc6G1LtFa1ztnXwdGtFWQ1vpVrXWm1jrTbrcfllABzTUAqGAxBIIgCOBbQ7AM6KOUSlNKBQOXArM8N1BKdfGYPRfY6EN5AAhoqqVGh0hoSBAEwYnPWg1prZuUUrcDc4AA4A2t9Xql1ONAltZ6FnCnUupcoAnYB1zrK3ksAppqqCZEWg0JgiA48ZkhANBazwZmt1j2iMf0g8CDvpShJQHNtdTqEELFDgiCIADHPll81AlsrqWGUOlZLAiC4MT/DEFTNTWEYBM7IAiCAPihIVCNJlkcGiQdygRBEMAPDUFzfRU6KJyIEJ+mRwRBEH4x+J0hUI3VBIVFHWsxBEEQjhv8yhA0NDkIaq4jLEIMgSAIgoVfGYKtRVWEUU9kVMyxFkUQBOG4wa8MweZdFYRTR0xM3LEWRRAE4bjBrwxBcVk5AUoTHR19rEURBEE4bvCrpjPN9WbAucDQyGMsiSD4L42NjRQUFFBXV3esRflVEhoaSnJyMkFBQR3ex68MgaOhCgBbiBgCQThWFBQUEBUVRY8ePaSH/xFGa01JSQkFBQWkpaV1eD+/Cg3pemMICAo/toIIgh9TV1dHQkKCGAEfoJQiISHhoL0t/zIEDSY0RHDEsRVEEPwcMQK+41CurV8ZAtVQbSbEEAiC31JWVsZLL710SPtOmTKFsrKyIyzRsce/DEGj0xBIaEgQ/Jb9GYKmpqb97jt79mxiY2N9IVaHaG5u9km5fmUIaJTQkCD4Ow888ABbt25l6NCh3HfffSxYsICTTjqJc889l4EDBwJw3nnnMWLECAYNGsSrr77q2rdHjx4UFxeTm5vLgAEDuOmmmxg0aBCnn346tbW1rY718ccfk56ezpAhQzj55JMBo8zvvfde0tPTycjI4J///CcA8+fPZ9iwYQwePJjrr7+e+vp61zHvv/9+hg8fzscff8zcuXMZO3Ysw4cP56KLLqKqquqwr4lftRqyNTpvlBgCQTgu+PPn69mws+KIljmwazSPnjOo3fVPPvkk69atY9WqVQAsWLCAFStWsG7dOldLmzfeeIP4+Hhqa2sZOXIkF1xwAQkJCV7lZGdn8/777/Paa69x8cUX88knn3DllVd6bfP4448zZ84cunXr5gopvfrqq+Tm5rJq1SoCAwPZt28fdXV1XHvttcyfP5++ffty9dVX8/LLL/O73/0OgISEBFasWEFxcTHnn38+8+bNIyIigr///e8888wzPPLIIxwOfuURWB+ul9CQpso66gAADZZJREFUIAiejBo1yqu55QsvvMCQIUMYM2YM+fn5ZGdnt9onLS2NoUOHAjBixAhyc3NbbTNu3DiuvfZaXnvtNVdYZ968edxyyy0EBpp6eHx8PJs3byYtLY2+ffsCcM011/DDDz+4yrnkkksAWLJkCRs2bGDcuHEMHTqUt956i7y8vMM+f7/yCIKbnC5UiAw6JwjHA/uruR9NIiLcUYIFCxYwb948Fi9eTHh4OBMmTGizOWZISIhrOiAgoM3Q0CuvvMLSpUv58ssvGTFiBMuXLz8s+bTWnHbaabz//vuHVE57+JVHENpcTb0KhYCO97gTBOHXRVRUFJWVle2uLy8vJy4ujvDwcDZt2sSSJUsO+Vhbt25l9OjRPP7449jtdvLz8znttNP497//7UpM79u3j379+pGbm0tOTg4A77zzDuPHj29V3pgxY/jpp59c21VXV7Nly5ZDls/CzwxBFXUBkh8QBH8mISGBcePGkZ6ezn333ddq/eTJk2lqamLAgAE88MADjBkz5pCPdd999zF48GDS09M54YQTGDJkCDfeeCOpqalkZGQwZMgQ3nvvPUJDQ5k+fToXXXQRgwcPxmazceutt7Yqz2638+abb3LZZZeRkZHB2LFj2bRp0yHLZ6G01oddyNEkMzNTZ2VlHdK+c/88maHBO+n04JojLJUgCB1l48aNDBgw4FiL8aumrWuslFqutc5sa3u/8gjCHdU0BMo4Q4IgCJ74jSHQWhOha2gUQyAIguCFTw2BUmqyUmqzUipHKfXAfra7QCmllVJtui1HgsZmTRQ1NAVJiyFBEARPfGYIlFIBwIvAmcBA4DKl1MA2tosC7gKW+koWgLqmZqJVDc3BYggEQRA88aVHMArI0Vpv01o3AB8AU9vY7i/A3wGffqWirrGZKMQQCIIgtMSXhqAbkO8xX+Bc5kIpNRxI0Vp/6UM5AKivqydMNaBD5MP1giAInhyzZLFSygY8A9zTgW1vVkplKaWyioqKDul4DdVmnA8dIt8rFgR/5nCGoQZ47rnnqKmpOYISHXt8aQgKgRSP+WTnMosoIB1YoJTKBcYAs9pKGGutX9VaZ2qtM+12+yEJ01RjDIEKFUMgCP7ML8UQaK1xOBw+Pw741hAsA/oopdKUUsHApcAsa6XWulxrnai17qG17gEsAc7VWh9ab7ED0FRbDoghEAR/p+Uw1ABPPfUUI0eOJCMjg0cffRQwwzecddZZDBkyhPT0dD788ENeeOEFdu7cycSJE5k4cWKbZQ8cOJCMjAzuvfdeAPbs2cO0adMYMmQIQ4YMYdGiRQA888wzpKenk56eznPPPQdAbm4u/fr14+qrryY9PZ38/Pw2ZTvS+GzQOa11k1LqdmAOEAC8obVer5R6HMjSWs/afwlHlmanRxAQduw+KiEIQgu+egB2rz2yZSYNhjOfbHd1y2Go586dS3Z2Nj///DNaa84991x++OEHioqK6Nq1K19+aVKY5eXlxMTE8Mwzz/Ddd9+RmJjoVW5JSQmffvopmzZtQinlGnb6zjvvZPz48Xz66ac0NzdTVVXF8uXLmT59OkuXLkVrzejRoxk/fjxxcXFkZ2fz1ltvMWbMmHZls75tcKTwaY5Aaz1ba91Xa91La/2Ec9kjbRkBrfUEX3kDAI46M+Z5QJgkiwVBcDN37lzmzp3LsGHDGD58OJs2bSI7O5vBgwfzzTffcP/997Nw4UJiYvavO2JiYggNDeWGG25g5syZhIeb4e6//fZbbrvtNsCMUhoTE8OPP/7ItGnTiIiIIDIykvPPP5+FCxcC0L17d9f4Ru3JdqTxm2Goda0xBIERYggE4bhhPzX3o4XWmgcffJBbbrml1boVK1Ywe/ZsHn74YSZNmrTfD8AEBgby888/M3/+fGbMmMG//vUvvv3224OWx3NI7P3JdiTxmyEmqDc5guBwCQ0Jgj/TchjqM844gzfeeMP1ycfCwkL27t3Lzp07CQ8P58orr+S+++5jxYoVbe5vUVVVRXl5OVOmTOHZZ59l9erVAEyaNImXX34ZMJ+pLC8v56STTuKzzz6jpqaG6upqPv30U0466aRWZbYn25HGbzyCalsUqxw96SoegfD/27v/2KrOOo7j7w+1cNk6h/vB+NGFlblEh5mAplndXIyLlfJPZ4KRqHMYSROdiftDIsv8gSb+oUQMJgvMhSVMiGPDERejiRs2LPtjMJyFsVW2ijOW4NDqEEyYyr7+cZ7itb23tLX03LPzeSU3Pfc5597z/fa59357nnP7HCu16mmou7q62LhxI/39/XR0dADQ0tLCjh07GBgYYN26dcyYMYPm5ubzH+Y9PT2sWLGCBQsW0Nvbe/55T58+TXd3N2fPniUi2LRpEwCbN2+mp6eHbdu20dTUxJYtW+jo6GDNmjW0t7cDsHbtWpYtWzbqKmednZ01Y5s7d+6U/k5KMw31g08f49s/7+fwhk7eXvGFaczy4mmoLz5PQ13Hoisvoes985jd3JR3KGZmDaU0Q0OdS+bRuWRe3mGYmTWc0hwRmJlZbS4EZjbtinZuskgm87t1ITCzaVWpVBgaGnIxuAgigqGhISqVyoQeV5pzBGbWGFpbWxkcHGSyMwnb2CqVCq2trRN6jAuBmU2r5uZm2tra8g7DqnhoyMys5FwIzMxKzoXAzKzkCjfFhKQ/A3+Y5MOvAv4yheHkybk0JufSmJwLLIqImpd4LFwh+H9IOlhvro2icS6Nybk0JucyNg8NmZmVnAuBmVnJla0Q/DDvAKaQc2lMzqUxOZcxlOocgZmZjVa2IwIzMxuhNIVA0gpJRyUNSFqfdzwTJelVSS9I6pN0MLVdIelJSa+kn+/IO85aJD0k6aSkI1VtNWNX5gepnw5LWp5f5KPVyWWDpOOpb/okraxad2/K5aikj+YT9WiSrpXUK+klSS9K+lJqL1y/jJFLEfulIumApEMpl2+m9jZJ+1PMuyTNTO2z0v2BtP66Se04It7yN6AJ+B2wGJgJHAJuzDuuCebwKnDViLbvAuvT8nrgO3nHWSf224DlwJELxQ6sBH4BCLgZ2J93/OPIZQPw5Rrb3phea7OAtvQabMo7hxTbfGB5Wr4MeDnFW7h+GSOXIvaLgJa03AzsT7/vR4HVqX0r8Pm0/AVga1peDeyazH7LckTQDgxExLGI+CfwCNCdc0xToRvYnpa3A3fkGEtdEfE08NcRzfVi7wYejsyzwBxJ86cn0gurk0s93cAjEfFGRPweGCB7LeYuIk5ExPNp+TTQDyykgP0yRi71NHK/REScSXeb0y2ADwO7U/vIfhnur93A7ZI00f2WpRAsBP5YdX+QsV8ojSiAX0r6taSe1HZNRJxIy38CrskntEmpF3tR++qLacjkoaohukLkkoYTlpH99VnofhmRCxSwXyQ1SeoDTgJPkh2xvB4R/06bVMd7Ppe0/hRw5UT3WZZC8FZwa0QsB7qAuyXdVr0ysmPDQn4FrMixJ1uA64GlwAnge/mGM36SWoCfAPdExN+r1xWtX2rkUsh+iYhzEbEUaCU7UnnXxd5nWQrBceDaqvutqa0wIuJ4+nkS2EP2Anlt+PA8/TyZX4QTVi/2wvVVRLyW3rxvAg/y32GGhs5FUjPZB+fOiHg8NReyX2rlUtR+GRYRrwO9QAfZUNzw9WOq4z2fS1p/OTA00X2VpRA8B9yQzrzPJDup8kTOMY2bpEslXTa8DHQCR8hyuCttdhfw03winJR6sT8BfCZ9S+Vm4FTVUEVDGjFW/jGyvoEsl9Xpmx1twA3AgemOr5Y0jrwN6I+ITVWrCtcv9XIpaL9cLWlOWp4NfITsnEcvsCptNrJfhvtrFfCrdCQ3MXmfJZ+uG9m3Hl4mG2+7L+94Jhj7YrJvORwCXhyOn2wscC/wCvAUcEXesdaJ/8dkh+b/Ihvf/Fy92Mm+NXF/6qcXgPfnHf84cvlRivVwemPOr9r+vpTLUaAr7/ir4rqVbNjnMNCXbiuL2C9j5FLEfrkJ+E2K+Qjw9dS+mKxYDQCPAbNSeyXdH0jrF09mv/7PYjOzkivL0JCZmdXhQmBmVnIuBGZmJedCYGZWci4EZmYl50JgNo0kfUjSz/KOw6yaC4GZWcm5EJjVIOnTaV74PkkPpInAzkj6fponfq+kq9O2SyU9myY321M1h/87JT2V5pZ/XtL16elbJO2W9FtJOyczW6TZVHIhMBtB0ruBTwC3RDb51zngU8ClwMGIWALsA76RHvIw8JWIuInsP1mH23cC90fEe4EPkP1HMmSzY95DNi/+YuCWi56U2RjeduFNzErnduB9wHPpj/XZZJOvvQnsStvsAB6XdDkwJyL2pfbtwGNpbqiFEbEHICLOAqTnOxARg+l+H3Ad8MzFT8usNhcCs9EEbI+Ie/+nUfraiO0mOz/LG1XL5/D70HLmoSGz0fYCqyTNhfPX8V1E9n4ZngHyk8AzEXEK+JukD6b2O4F9kV0pa1DSHek5Zkm6ZFqzMBsn/yViNkJEvCTpq2RXhJtBNtPo3cA/gPa07iTZeQTIpgHemj7ojwGfTe13Ag9I+lZ6jo9PYxpm4+bZR83GSdKZiGjJOw6zqeahITOzkvMRgZlZyfmIwMys5FwIzMxKzoXAzKzkXAjMzErOhcDMrORcCMzMSu4/wgsp3eWisr8AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' develop 01 '''\n",
        "sgd_model = SGDClassifier(max_iter=100, tol=None, random_state=32)\n",
        "\n",
        "# 학습 \n",
        "sgd_model.fit(X_train3, y_train3)\n",
        "\n",
        "# 예측\n",
        "y_pred_24 = sgd_model.predict(X_test3)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test3, y_pred_24)\n",
        "print(f'► SGD Classifier의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351bf74d-b218-44ca-ddf1-bd8f0244c35a",
        "id": "Jzv6APAPsndr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► SGD Classifier의 정확도는 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "해당 모델의 정확도를 더 높일 수 있는 방안은 없을까? 모델이 예측한 확률 값을 직접적으로 반영하여 평가하는 Log loss를 적용해보자.\n",
        "\n",
        "아래와 같이, 정확도가 97.4%로 증가했다. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JCGP-lIfvZW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' develop 02 '''\n",
        "sgd_model = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=32)\n",
        "\n",
        "# 학습 \n",
        "sgd_model.fit(X_train3, y_train3)\n",
        "\n",
        "# 예측\n",
        "y_pred_24 = sgd_model.predict(X_test3)\n",
        "\n",
        "# 정확도\n",
        "accuracy = accuracy_score(y_test3, y_pred_24)\n",
        "print(f'► SGD Classifier의 정확도는 {accuracy}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3756397f-a989-4852-cb85-4c87a2e4c44c",
        "id": "GTgTe9HytluC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "► SGD Classifier의 정확도는 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "\n",
        "###**소 결**\n",
        "\n"
      ],
      "metadata": {
        "id": "1ghEis01eVzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5가지 모델을 통해 유방암을 진단한 결과는 다음과 같다.\n",
        "\n",
        "*   **Decision Tree**\n",
        "    *   accuracy : 0.95\n",
        "    *   f1-score : (min)0.93,  (max)0.96\n",
        "    *   recall : (min)0.93,  (max)0.96\n",
        "*   **Random Forest**\n",
        "    *   accuracy : 0.96\n",
        "    *   f1-score : (min)0.95,  (max)0.97\n",
        "    *   recall : (min)0.93,  (max)0.99\n",
        "*   **Support Vector Machine(SVM)**\n",
        "    *   accuracy : 0.95\n",
        "    *   f1-score : (min)0.92,  (max)0.96\n",
        "    *   recall : (min)0.86,  (max)1.00\n",
        "*   **SGD Classifier**\n",
        "    *   accuracy : 0.96\n",
        "    *   f1-score : (min)0.95,  (max)0.96\n",
        "    *   recall : (min)0.93,  (max)1.00\n",
        "*   **Logistic Regression**\n",
        "    *   accuracy : 0.96\n",
        "    *   f1-score : (min)0.94,  (max)0.97\n",
        "    *   recall : (min)0.91,  (max)0.99\n",
        "\n",
        "<br>\n",
        "\n",
        "유방암 진단은 악성 환자를 놓치는 FN을 낮추는 것이 중요하므로 재현률(Recall)을 추가 평가지표로 선택했다. 따라서 정밀도와 F1-score 높은 모델들 가운데 Recall값이 가장 높게 나온 **SGD Classifier**가 적합한 모델이라고 판단된다.\n",
        "\n",
        "기본적인 SGD Classifier 모델의 학습에 Epoch 값을 조정하여 성능을 개선할 수 있으며, Log loss을 적용하여 정확도를 증가시킬 수 있었다. 정확도 95.6%에서 Epoch 적용(100) 및 Log loss로 최종 97.4%까지 성능을 향상할 수 있었다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br><br><br><br><br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "-bMu6wnoeVzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **III. 결 론**\n",
        "\n",
        "<br>\n",
        "\n",
        "(1) 손글씨 이미지 판별, (2) 와인 종류 판별, (3) 유방암 여부 진단, 총 3개의 문제를 해결하기 위해 해당 데이터를 import한 후 분석을 실시하고, 제시된 5가지의 Classification model을 기본으로 학습 및 테스트를 수행하였다. 또한 수행 결과의 평가를 위한 지표를 검토하여 가장 적정한 모델을 선택했다. \n",
        "\n",
        "이 과정에서 몇 가지 어려움이 있었다. 과적합이 발생한 경우가 있었고, 정확도(accuracy)가 유사한 모델 간 선택이 문제이기도 했다. 검증점수를 더 높일 수 있는 방법도 고민 되었다.\n",
        "\n",
        "이를 해결하고자 추가적으로 과적합 여부 분석, 대안모델 검토, 규제적용 등을 실시했다. 또한 동일 모델에 Epoch를 조정한 결과를 비교하여 성능에 미치는 영향을 검토하였으며, Log loss를 통해 예측값의 정답확률을 높이기도 했다. 모든 시도가 성능개선에 유효한 역할을 한 것은 아니었지만 보다 나은 문제해결 방안을 찾기위한 노력이었다. 이때 데이터별 검증점수를 시각화하여 눈으로 확인하는 것이 대안을 모색하는 데 도움이 되었다.  \n",
        "\n",
        "세 문제 모두 데이터 셋팅 및 학습은 유사한 구조로 진행됐지만 학습의 결과 및 적정 평가지표는 각각 달랐다. 결과값을 이해하고 평가를 올바르게 하기 위해서는 무엇보다 원 데이터의 이해가 중요하다는 생각이 든다. 데이터에 대한 이해가 선행되어야 해결하고자 하는 문제가 무엇인지 정확히 인식할 수 있고, 그래야 적절한 평가도 가능하다.\n",
        "\n",
        "<br>\n",
        "\n",
        "**[향후 과제]**\n",
        "\n",
        "훈련 데이터의 검증점수가 만점(1.0)인 것 자체가 과대적합(overfitting)으로 해석되어야 하는지 판단하기 어려웠다. 특히 테스트 데이터의 점수는 적정한 차이를 두고 훈련 데이터 점수를 하회하는 경우, 성능이 좋다고 판단하여도 되는지에 관한 배경지식이 아직은 부족하다.\n",
        "\n",
        "규제적용으로 조절되지 않은 과소적합(underfitting)을 해소할 방법을 알아내는 것도 향후 과제이다. 개념적으로는 훈련모델을 보다 고도화하는 것이 하나의 방법이 될 수 있다고 생각되는데 실제로 어떤 대안이 있는지 추가연구가 필요하다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1GyE3sDpSy4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "            \n",
        "<br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "< 참고문헌 >\n",
        "\n",
        "- ⌈혼자 공부하는 머신러닝 딥러닝⌋, 한빛미디어, 박해선 지음\n",
        "\n",
        "<br><br>\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VvtvwZRiR-eU"
      }
    }
  ]
}